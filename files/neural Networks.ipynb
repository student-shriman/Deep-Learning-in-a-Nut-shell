{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# What is Deep Learning ??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To understand what Deep Learning is, we first need to understand the relationship that Deep learning has with Machine learning,\n",
    "     neural networks, and artificial intelligence.\n",
    " \n",
    " The best way to think of this relationship is to visualize them as concentric circles:\n",
    "<img src='https://www.safaribooksonline.com/library/view/deep-learning/9781491924570/assets/dpln_0101.png'>\n",
    "\n",
    "\n",
    "  At the outer most ring you have Artificial Intelligence.\n",
    "    Second layer inside of Artificial Intelligence is Machine learning and Deep learning is inside the machine learning at the centre.\n",
    "    \n",
    "    Broadly speaking, deep learning is a more approachable name for an artificial neural network. \n",
    "    The “deep” in deep learning refers to the depth of the network. An artificial neural network can be very shallow.\n",
    "\n",
    "\n",
    " Neural networks are inspired by the structure of the cerebral cortex.\n",
    "     At the basic level is the perceptron, the mathematical representation of a biological neuron. \n",
    "         Like in the cerebral cortex, there can be several layers of interconnected perceptrons.\n",
    "\n",
    "\n",
    " The first layer is the input layer. Each node in this layer takes an input, and then passes its output as the input\n",
    "        to each node in the next layer. There are generally no connections between nodes in the same layer and \n",
    "             the last layer produces the outputs.\n",
    "\n",
    "\n",
    " We call the middle part the hidden layer. These neurons have no connection to the outside (e.g. input or output) and \n",
    "   are only activated by nodes in the previous layer.\n",
    "<img src='http://neuralnetworksanddeeplearning.com/images/tikz11.png'>\n",
    "\n",
    "\n",
    " Think of deep learning as the technique for learning in neural networks that utilizes multiple layers of abstraction \n",
    "     to solve pattern recognition problems. In the 1980s, most neural networks were a single layer due to the cost of \n",
    "         computation and availability of data.\n",
    "\n",
    "\n",
    "Machine learning is considered a branch or approach of Artificial intelligence, whereas deep learning is a specialized type\n",
    "  of machine learning.\n",
    "\n",
    "\n",
    "Machine learning involves computer intelligence that doesn’t know the answers up front. Instead, the program will run \n",
    "   against training data, verify the success of its attempts, and modify its approach accordingly.\n",
    "   \n",
    "Machine learning typical requires a sophisticated education, spanning software engineering and computer science\n",
    "     to statistical methods and linear algebra.\n",
    "\n",
    "\n",
    "There are two broad classes of machine learning methods:\n",
    "\n",
    "   * Supervised learning\n",
    "   * Unsupervised learning\n",
    "\n",
    "\n",
    "In supervised learning, a machine learning algorithm uses a labeled dataset to infer the desired outcome. This takes a lot of data and time, since the data needs to be labeled by hand. Supervised learning is great for classification and regression problems.\n",
    "\n",
    "For example, let’s say that we were running a company and want to determine the effect of bonuses on employee retention. If we had historical data – i.e. employee bonus amount and tenure – we could use supervised machine learning.\n",
    "\n",
    "With unsupervised learning, there aren’t any predefined or corresponding answers. The goal is to figure out the hidden patterns in the data. It’s usually used for clustering and associative tasks, like grouping customers by behavior. Amazon’s “customers who also bought…” recommendations are a type of associative task.\n",
    "\n",
    "While supervised learning can be useful, we often have to resort to unsupervised learning. Deep learning has proven to be an effective unsupervised learning technique.\n",
    "\n",
    "## Why is Deep Learning Important?\n",
    "\n",
    "<img src='https://image.slidesharecdn.com/andrew-ng-extract-oct2015-nonotes-151124104249-lva1-app6891/95/andrew-ng-chief-scientist-at-baidu-30-638.jpg?cb=1448361887'>\n",
    "\n",
    "Computers have long had techniques for recognizing features inside of images. The results weren’t always great. Computer vision has been a main beneficiary of deep learning. Computer vision using deep learning now rivals humans on many image recognition tasks.\n",
    "\n",
    "\n",
    "Facebook has had great success with identifying faces in photographs by using deep learning. It’s not just a marginal improvement, but a game changer: “Asked whether two unfamiliar photos of faces show the same person, a human being will get it right 97.53 percent of the time. New software developed by researchers at Facebook can score 97.25 percent on the same challenge, regardless of variations in lighting or whether the person in the picture is directly facing the camera.”\n",
    "\n",
    "\n",
    "Speech recognition is a another area that’s felt deep learning’s impact. Spoken languages are so vast and ambiguous. Baidu – one of the leading search engines of China – has developed a voice recognition system that is faster and more accurate than humans at producing text on a mobile phone. In both English and Mandarin.\n",
    "\n",
    "\n",
    "What is particularly fascinating, is that generalizing the two languages didn’t require much additional design effort: “Historically, people viewed Chinese and English as two vastly different languages, and so there was a need to design very different features,” Andrew Ng says, chief scientist at Baidu. “The learning algorithms are now so general that you can just learn.”\n",
    "\n",
    "Google is now using deep learning to manage the energy at the company’s data centers. They’ve cut their energy needs for cooling by 40%. That translates to about a 15% improvement in power usage efficiency for the company and hundreds of millions of dollars in savings.\n",
    "\n",
    "## Deep Learning Microservices\n",
    "\n",
    "Here’s a quick overview of some deep learning use cases and microservices.\n",
    "\n",
    "Illustration Tagger. An implementation of Illustration2Vec, this microservice can tag an image with the safe, questionable, or explicit rating, the copyright, and general category tag to understand what’s in the image. DeepFilter is a style transfer service for applying artistic filters to images.\n",
    "\n",
    "The age classifier uses face detection to determine the age of a person in a photo. The Places 365 Classifier uses a pre-trained CNN and based on Places: An Image Database for Deep Scene Understanding B. Zhou, et al., 2016 to identify particular locations in images, such as a courtyard, drugstore, hotel room, glacier, mountain, etc. Lastly, there is InceptionNet, a direct implementation of Google’s InceptionNet using TensorFlow. It takes an image (such as a car), and returns the top 5 classes the model predicts are relevant to the image.\n",
    "\n",
    "## Open Source Deep Learning Frameworks\n",
    "\n",
    "Deep learnings is made accessible by a number of open source projects. Some of the most popular technologies include, but are not limited to, Deeplearning4j (DL4j), Theano, Torch, TensorFlow, and Caffe. The deciding factors on which one to use are the tech stack they target, and if they are low-level, academic, or application focused. Here’s an overview of each:\n",
    "\n",
    "DL4J:\n",
    "\n",
    "   * JVM-based\n",
    "   * Distrubted\n",
    "   * Integrates with Hadoop and Spark\n",
    "   \n",
    "   \n",
    "Theano:\n",
    "\n",
    "   * Very popular in Academia\n",
    "   * Fairly low level\n",
    "   * Interfaced with via Python and Numpy\n",
    "\n",
    "\n",
    "Torch:\n",
    "\n",
    "   * Lua based\n",
    "   * In house versions used by Facebook and Twitter\n",
    "   * Contains pretrained models\n",
    "\n",
    "\n",
    "TensorFlow:\n",
    "\n",
    "   * Google written successor to Theano\n",
    "   * Interfaced with via Python and Numpy\n",
    "   * Highly parallel\n",
    "   * Can be somewhat slow for certain problem sets\n",
    "\n",
    "\n",
    "\n",
    "Caffe:\n",
    "\n",
    "   * Not general purpose. Focuses on machine-vision problems\n",
    "   * Implemented in C++ and is very fast\n",
    "   * Not easily extensible\n",
    "   * Has a Python interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## McCulloch and Pitts Neuron\n",
    "\n",
    "In 1943, McCulloch and Pitts introduced a mathematical model of a neuron. It consisted of three components:\n",
    "\n",
    "1. A set of **weights** $w_i$ corresponding to synapses (inputs)\n",
    "2. An **adder** for summing input signals; analogous to cell membrane that collects charge\n",
    "3. An **activation function** for determining when the neuron fires, based on accumulated input\n",
    "\n",
    "The neuron model is shown schematically below. On the left are input nodes $\\{x_i\\}$, usually expressed as a vector. The strength with which the inputs are able to deliver the signal along the synapse is determined by their corresponding weights $\\{w_i\\}$. The adder then sums the inputs from all the synapses:\n",
    "\n",
    "$$h = \\sum_i w_i x_i$$\n",
    "\n",
    "The parameter $\\theta$ determines whether or not the neuron fires given a weighted input of $h$. If it fires, it returns a value $y=1$, otherwise $y=0$. For example, a simple **activation function** is using $\\theta$ as a simple fixed threshold:\n",
    "\n",
    "$$y = g(h) = \\left\\{ \\begin{array}{l}\n",
    "1, \\text{if } h \\gt \\theta \\\\\n",
    "0, \\text{if } h \\le \\theta\n",
    "\\end{array} \\right.$$\n",
    "\n",
    "this activation function may take any of several forms, such as a logistic function.\n",
    "\n",
    "![neuron](http://d.pr/i/9AMK+)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single neuron is not interesting, nor useful, from a learning perspective. It cannot learn; it simply receives inputs and either fires or not. Only when neurons are joined as a **network** can they perform useful work.\n",
    "\n",
    "Learning takes place by changing the weights of the connections in a neural network, and by changing the parameters of the activation functions of neurons.\n",
    "\n",
    "## Perceptron\n",
    "\n",
    "A collection of McCullough and Pitts neurons, along with a set of input nodes connected to the inputs via weighted edges, is a perceptron, the simplest neural network.\n",
    "\n",
    "Each neuron is independent of the others in the perceptron, in the sense that its behavior and performance depends only on its own weights and threshold values, and not of those for the other neurons. Though they share inputs, they operate independently.\n",
    "\n",
    "The number of inputs and outputs are determined by the data. Weights are stored as a `N x K` matrix, with N observations and K neurons, with $w_{ij}$ specifying the weight on the *i*th observation on the *j*th neuron.\n",
    "\n",
    "<img src='http://d.pr/i/4IWA+'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the perceptron for statistical learning, we compare the outputs $y_j$ from each neuron to the obervation target $t_j$, and adjust the input weights when they do not correspond (*e.g.* if a neuron fires when it should not have).\n",
    "\n",
    "$$t_j - y_j$$\n",
    "\n",
    "We use this difference to update the weight $w_{ij}$, based on the input and a desired **learning rate**. This results in an update rule:\n",
    "\n",
    "$$w_{ij} \\leftarrow w_{ij} + \\eta (t_j - y_j) x_i$$\n",
    "\n",
    "After an incremental improvement, the perceptron is shown the training data again, resulting in another update. This is repeated until the performance no longer improves. Having a learning rate less than one results in a more stable learning rate, though this stability is traded off against having to expose the network to the data multiple times. Typical learning rates are in the 0.1-0.4 range.\n",
    "\n",
    "An additional input node is typically added to the perceptron model, which is a constant value (usually -1, 0, or 1) that acts analogously to an intercept in a regression model. This establishes a baseline input for the case when all inputs are zero.\n",
    "\n",
    "![bias](http://d.pr/i/105b5+)\n",
    "\n",
    "## Learning with Perceptrons\n",
    "\n",
    "1. Initialize weights $w_{ij}$ to small, random numbers.\n",
    "2. For each t in T iterations\n",
    "    * compute activation for each neuron *j* connected to each input vector *i*\n",
    "    $$y_j = g\\left( h=\\sum_i w_{ij} x_i \\right) = \\left\\{ \\begin{array}{l}\n",
    "1, \\text{if } h \\gt 0 \\\\\n",
    "0, \\text{if } h \\le 0\n",
    "\\end{array} \\right.$$\n",
    "    * update weights\n",
    "    $$w_{ij} \\leftarrow w_{ij} + \\eta (t_j - y_j) x_i$$\n",
    "\n",
    "\n",
    "This algorithm is $\\mathcal{O}(Tmn)$\n",
    "\n",
    "### Example: Logical functions\n",
    "\n",
    "Let's see how the perceptron learns by training it on a couple of of logical functions, AND and OR. For two variables `x1` and `x2`, the AND function returns 1 if both are true, or zero otherwise; the OR function returns 1 if either variable is true, or both. These functions can be expressed as simple lookup tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from scipy import optimize\n",
    "from ipywidgets import *\n",
    "from IPython.display import SVG\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x1  x2  y\n",
       "0   0   0  0\n",
       "1   0   1  0\n",
       "2   1   0  0\n",
       "3   1   1  1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AND = pd.DataFrame({'x1': (0,0,1,1), 'x2': (0,1,0,1), 'y': (0,0,0,1)})\n",
    "AND"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to initialize weights to small, random values (can be positive and negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.randn(3)*1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, a simple activation function for calculating $g(h)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = lambda inputs, weights: np.where(np.dot(inputs, weights)>0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a training function that iterates the learning algorithm, returning the adapted weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(inputs, targets, weights, eta, n_iterations):\n",
    "\n",
    "    # Add the inputs that match the bias node\n",
    "    inputs = np.c_[inputs, -np.ones((len(inputs), 1))]\n",
    "\n",
    "    for n in range(n_iterations):\n",
    "\n",
    "        activations = g(inputs, weights);\n",
    "        weights -= eta*np.dot(np.transpose(inputs), activations - targets)\n",
    "        \n",
    "    return(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it first on the AND function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = AND[['x1','x2']]\n",
    "target = AND['y']\n",
    "\n",
    "w = train(inputs, target, w, 0.25, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g(np.c_[inputs, -np.ones((len(inputs), 1))], w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, it has learned the function perfectly. Now for OR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   x1  x2  y\n",
       "0   0   0  0\n",
       "1   0   1  1\n",
       "2   1   0  1\n",
       "3   1   1  1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OR = pd.DataFrame({'x1': (0,0,1,1), 'x2': (0,1,0,1), 'y': (0,1,1,1)})\n",
    "OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.randn(3)*1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = OR[['x1','x2']]\n",
    "target = OR['y']\n",
    "\n",
    "w = train(inputs, target, w, 0.25, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g(np.c_[inputs, -np.ones((len(inputs), 1))], w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also 100% correct.\n",
    "\n",
    "### Exercise: XOR\n",
    "\n",
    "Now try running the model on the XOR function, where a one is returned for either `x1` or `x2` being true, but *not* both. What happens here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the problem graphically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "AND.plot(kind='scatter', x='x1', y='x2', c='y', s=50, colormap='winter')\n",
    "plt.plot(np.linspace(0,1.4), 1.5 - 1*np.linspace(0,1.4), 'k--');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "OR.plot(kind='scatter', x='x1', y='x2', c='y', s=50, colormap='winter')\n",
    "plt.plot(np.linspace(-.4,1), .5 - 1*np.linspace(-.4,1), 'k--');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "XOR = pd.DataFrame({'x1': (0,0,1,1), 'x2': (0,1,0,1), 'y': (0,1,1,0)})\n",
    "\n",
    "XOR.plot(kind='scatter', x='x1', y='x2', c='y', s=50, colormap='winter');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron tries to find a separating hyperplane for the two response classes. Namely, a set of weights that satisfies:\n",
    "\n",
    "$$\\mathbf{x_1}\\mathbf{w}^T=0$$\n",
    "\n",
    "and:\n",
    "\n",
    "$$\\mathbf{x_2}\\mathbf{w}^T=0$$\n",
    "\n",
    "Hence,\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf{x}_1\\mathbf{w}^T &= \\mathbf{x}_2\\mathbf{w}^T \\\\\n",
    "\\Rightarrow (\\mathbf{x}_1 - \\mathbf{x}_2) \\mathbf{w}^T &= 0\n",
    "\\end{aligned}$$\n",
    "\n",
    "This means that either the norms of $\\mathbf{x}_1 - \\mathbf{x}_2$ or $\\mathbf{w}$ are zero, or the cosine of the angle between them is equal to zero, due to the identity:\n",
    "\n",
    "$$\\mathbf{a}\\mathbf{b} = \\|a\\| \\|b\\| \\cos \\theta$$\n",
    "\n",
    "Since there is no reason for the norms to be zero in general, we need the two vectors to be at right angles to one another. So, we need a weight vector that is perpendicular to the decision boundary.\n",
    "\n",
    "Clearly, for the XOR function, the output classes are not linearly separable. So, the algorithm does not converge on an answer, but simply cycles through two incorrect solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron\n",
    "\n",
    "The solution to fitting more complex (*i.e.* non-linear) models with neural networks is to use a more complex network that consists of more than just a single perceptron. The take-home message from the perceptron is that all of the learning happens by adapting the synapse weights until prediction is satisfactory. Hence, a reasonable guess at how to make a perceptron more complex is to simply **add more weights**.\n",
    "\n",
    "There are two ways to add complexity:\n",
    "\n",
    "1. Add backward connections, so that output neurons feed back to input nodes, resulting in a **recurrent network**\n",
    "2. Add neurons between the input nodes and the outputs, creating an additional (\"hidden\") layer to the network, resulting in a **multi-layer perceptron**\n",
    "\n",
    "The latter approach is more common in applications of neural networks.\n",
    "\n",
    "<imng src='http://d.pr/i/14BS1+'>\n",
    "\n",
    "How to train a multilayer network is not intuitive. Propagating the inputs forward over two layers is straightforward, since the outputs from the hidden layer can be used as inputs for the output layer. However, the process for updating the weights based on the prediction error is less clear, since it is difficult to know whether to change the weights on the input layer or on the hidden layer in order to improve the prediction.\n",
    "\n",
    "Updating a multi-layer perceptron (MLP) is a matter of: \n",
    "\n",
    "1. moving forward through the network, calculating outputs given inputs and current weight estimates\n",
    "2. moving backward updating weights according to the resulting error from forward propagation. \n",
    "\n",
    "In this sense, it is similar to a single-layer perceptron, except it has to be done twice, once for each layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "Backpropagation is a method for efficiently computing the gradient of the cost function of a neural network with respect to its parameters.  These partial derivatives can then be used to update the network's parameters using, e.g., gradient descent.  This may be the most common method for training neural networks.  Deriving backpropagation involves numerous clever applications of the chain rule for functions of vectors. \n",
    "\n",
    "\n",
    "<img src='https://theclevermachine.files.wordpress.com/2014/09/neural-net.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review: The chain rule\n",
    "\n",
    "The chain rule is a way to compute the derivative of a function whose variables are themselves functions of other variables.  If $C$ is a scalar-valued function of a scalar $z$ and $z$ is itself a scalar-valued function of another scalar variable $w$, then the chain rule states that\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w} = \\frac{\\partial C}{\\partial z}\\frac{\\partial z}{\\partial w}\n",
    "$$\n",
    "For scalar-valued functions of more than one variable, the chain rule essentially becomes additive.  In other words, if $C$ is a scalar-valued function of $N$ variables $z_1, \\ldots, z_N$, each of which is a function of some variable $w$, the chain rule states that\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w} = \\sum_{i = 1}^N \\frac{\\partial C}{\\partial z_i}\\frac{\\partial z_i}{\\partial w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "In the following derivation, we'll use the following notation:\n",
    "\n",
    "$L$ - Number of layers in the network.\n",
    "\n",
    "$N^n$ - Dimensionality of layer $n \\in \\{0, \\ldots, L\\}$.  $N^0$ is the dimensionality of the input; $N^L$ is the dimensionality of the output.\n",
    "\n",
    "$W^m \\in \\mathbb{R}^{N^m \\times N^{m - 1}}$ - Weight matrix for layer $m \\in \\{1, \\ldots, L\\}$.  $W^m_{ij}$ is the weight between the $i^{th}$ unit in layer $m$ and the $j^{th}$ unit in layer $m - 1$.\n",
    "\n",
    "$b^m \\in \\mathbb{R}^{N^m}$ - Bias vector for layer $m$.\n",
    "\n",
    "$\\sigma^m$ - Nonlinear activation function of the units in layer $m$, applied elementwise.\n",
    "\n",
    "$z^m \\in \\mathbb{R}^{N^m}$ - Linear mix of the inputs to layer $m$, computed by $z^m = W^m a^{m - 1} + b^m$.\n",
    "\n",
    "$a^m \\in \\mathbb{R}^{N^m}$ - Activation of units in layer $m$, computed by $a^m = \\sigma^m(h^m) = \\sigma^m(W^m a^{m - 1} + b^m)$.  $a^L$ is the output of the network.  We define the special case $a^0$ as the input of the network.\n",
    "\n",
    "$y \\in \\mathbb{R}^{N^L}$ - Target output of the network.\n",
    "\n",
    "$C$ - Cost/error function of the network, which is a function of $a^L$ (the network output) and $y$ (treated as a constant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation in general\n",
    "\n",
    "In order to train the network using a gradient descent algorithm, we need to know the gradient of each of the parameters with respect to the cost/error function $C$; that is, we need to know $\\frac{\\partial C}{\\partial W^m}$ and $\\frac{\\partial C}{\\partial b^m}$.  It will be sufficient to derive an expression for these gradients in terms of the following terms, which we can compute based on the neural network's architecture:\n",
    "\n",
    "- $\\frac{\\partial C}{\\partial a^L}$: The derivative of the cost function with respect to its argument, the output of the network\n",
    "- $\\frac{\\partial a^m}{\\partial z^m}$: The derivative of the nonlinearity used in layer $m$ with respect to its argument\n",
    "\n",
    "To compute the gradient of our cost/error function $C$ to $W^m_{ij}$ (a single entry in the weight matrix of the layer $m$), we can first note that $C$ is a function of $a^L$, which is itself a function of the linear mix variables $z^m_k$, which are themselves functions of the weight matrices $W^m$ and biases $b^m$.  With this in mind, we can use the chain rule as follows:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial W^m_{ij}} = \\sum_{k = 1}^{N^m} \\frac{\\partial C}{\\partial z^m_k} \\frac{\\partial z^m_k}{\\partial W^m_{ij}}$$\n",
    "\n",
    "Note that by definition \n",
    "$$\n",
    "z^m_k = \\sum_{l = 1}^{N^m} W^m_{kl} a_l^{m - 1} + b^m_k\n",
    "$$\n",
    "It follows that $\\frac{\\partial z^m_k}{\\partial W^m_{ij}}$ will evaluate to zero when $i \\ne k$ because $z^m_k$ does not interact with any elements in $W^m$ except for those in the $k$<sup>th</sup> row, and we are only considering the entry $W^m_{ij}$.  When $i = k$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial z^m_i}{\\partial W^m_{ij}} &= \\frac{\\partial}{\\partial W^m_{ij}}\\left(\\sum_{l = 1}^{N^m} W^m_{il} a_l^{m - 1} + b^m_i\\right)\\\\\n",
    "&= a^{m - 1}_j\\\\\n",
    "\\rightarrow \\frac{\\partial z^m_k}{\\partial W^m_{ij}} &= \\begin{cases}\n",
    "0 & k \\ne i\\\\\n",
    "a^{m - 1}_j & k = i\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "The fact that $\\frac{\\partial C}{\\partial a^m_k}$ is $0$ unless $k = i$ causes the summation above to collapse, giving\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial W^m_{ij}} = \\frac{\\partial C}{\\partial z^m_i} a^{m - 1}_j$$\n",
    "\n",
    "or in vector form\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial W^m} = \\frac{\\partial C}{\\partial z^m} a^{m - 1 \\top}$$\n",
    "\n",
    "Similarly for the bias variables $b^m$, we have\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^m_i} = \\sum_{k = 1}^{N^m} \\frac{\\partial C}{\\partial z^m_k} \\frac{\\partial z^m_k}{\\partial b^m_i}$$\n",
    "\n",
    "As above, it follows that $\\frac{\\partial z^m_k}{\\partial b^m_i}$ will evaluate to zero when $i \\ne k$ because $z^m_k$ does not interact with any element in $b^m$ except $b^m_k$.  When $i = k$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial z^m_i}{\\partial b^m_i} &= \\frac{\\partial}{\\partial b^m_i}\\left(\\sum_{l = 1}^{N^m} W^m_{il} a_l^{m - 1} + b^m_i\\right)\\\\\n",
    "&= 1\\\\\n",
    "\\rightarrow \\frac{\\partial z^m_i}{\\partial b^m_i} &= \\begin{cases}\n",
    "0 & k \\ne i\\\\\n",
    "1 & k = i\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "The summation also collapses to give\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^m_i} = \\frac{\\partial C}{\\partial z^m_i}$$\n",
    "\n",
    "or in vector form\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^m} = \\frac{\\partial C}{\\partial z^m}$$\n",
    "\n",
    "Now, we must compute $\\frac{\\partial C}{\\partial z^m_k}$.  For the final layer ($m = L$), this term is straightforward to compute using the chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial z^L_k} = \\frac{\\partial C}{\\partial a^L_k} \\frac{\\partial a^L_k}{\\partial z^L_k}\n",
    "$$\n",
    "\n",
    "or, in vector form\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial z^L} = \\frac{\\partial C}{\\partial a^L} \\frac{\\partial a^L}{\\partial z^L}\n",
    "$$\n",
    "\n",
    "The first term $\\frac{\\partial C}{\\partial a^L}$ is just the derivative of the cost function with respect to its argument, whose form depends on the cost function chosen.  Similarly, $\\frac{\\partial a^m}{\\partial z^m}$ (for any layer $m$ includling $L$) is the derivative of the layer's nonlinearity with respect to its argument and will depend on the choice of nonlinearity.  For other layers, we again invoke the chain rule:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial C}{\\partial z^m_k} &= \\frac{\\partial C}{\\partial a^m_k} \\frac{\\partial a^m_k}{\\partial z^m_k}\\\\\n",
    "&= \\left(\\sum_{l = 1}^{N^{m + 1}}\\frac{\\partial C}{\\partial z^{m + 1}_l}\\frac{\\partial z^{m + 1}_l}{\\partial a^m_k}\\right)\\frac{\\partial a^m_k}{\\partial z^m_k}\\\\\n",
    "&= \\left(\\sum_{l = 1}^{N^{m + 1}}\\frac{\\partial C}{\\partial z^{m + 1}_l}\\frac{\\partial}{\\partial a^m_k} \\left(\\sum_{h = 1}^{N^m} W^{m + 1}_{lh} a_h^m + b_l^{m + 1}\\right)\\right) \\frac{\\partial a^m_k}{\\partial z^m_k}\\\\\n",
    "&= \\left(\\sum_{l = 1}^{N^{m + 1}}\\frac{\\partial C}{\\partial z^{m + 1}_l} W^{m + 1}_{lk}\\right) \\frac{\\partial a^m_k}{\\partial z^m_k}\\\\\n",
    "&= \\left(\\sum_{l = 1}^{N^{m + 1}}W^{m + 1\\top}_{kl} \\frac{\\partial C}{\\partial z^{m + 1}_l}\\right) \\frac{\\partial a^m_k}{\\partial z^m_k}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "where the last simplification was made because by convention $\\frac{\\partial C}{\\partial z^{m + 1}_l}$ is a column vector, allowing us to write the following vector form:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial z^m} = \\left(W^{m + 1\\top} \\frac{\\partial C}{\\partial z^{m + 1}}\\right) \\circ \\frac{\\partial a^m}{\\partial z^m}$$\n",
    "\n",
    "Note that we now have the ingredients to efficiently compute the gradient of the cost function with respect to the network's parameters:  First, we compute $\\frac{\\partial C}{\\partial z^L_k}$ based on the choice of cost function and nonlinearity.  Then, we recursively can compute $\\frac{\\partial C}{\\partial z^m}$ layer-by-layer based on the term $\\frac{\\partial C}{\\partial z^{m + 1}}$ computed from the previous layer and the nonlinearity of the layer (this is called the \"backward pass\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation in practice\n",
    "\n",
    "As discussed above, the exact form of the updates depends on both the chosen cost function and each layer's chosen nonlinearity.  The following two table lists the some common choices for nonlinearities and the required partial derivative for deriving the gradient for each layer:\n",
    "\n",
    "| Nonlinearity | $a^m = \\sigma^m(z^m)$ | $\\frac{\\partial a^m}{\\partial z^m}$ | Notes |\n",
    "|--------------|---|---|---|\n",
    "| Sigmoid      | $\\frac{1}{1 + e^{z^m}}$ | $\\sigma^m(z^m)(1 - \\sigma^m(z^m)) = a^m(1 - a^m)$ | \"Squashes\" any input to the range $[0, 1]$ |\n",
    "| Tanh         | $\\frac{e^{z^m} - e^{-z^m}}{e^{z^m} + e^{-z^m}}$ | $1 - (\\sigma^m(z^m))^2 = 1 - (a^m)^2$ | Equivalent, up to scaling, to the sigmoid function |\n",
    "| ReLU         | $\\max(0, z^m)$ | $0, z^m < 0;\\; 1, z^m \\ge 0$ | Commonly used in neural networks with many layers|\n",
    "\n",
    "Similarly, the following table collects some common cost functions and the partial derivative needed to compute the gradient for the final layer:\n",
    "\n",
    "| Cost Function | $C$                                  | $\\frac{\\partial C}{\\partial a^L}$ | Notes |\n",
    "|---------------|--------------------------------------|-----------------------------------|---|\n",
    "| Squared Error | $\\frac{1}{2}(y - a^L)^\\top(y - a^L)$ | $y - a^L$                         | Commonly used when the output is not constrained to a specific range |\n",
    "| Cross-Entropy | $(y - 1)\\log(1 - a^L) - y\\log(a^L)$  | $\\frac{a^L - y}{a^L(1 - a^L)}$    | Commonly used for binary classification tasks; can yield faster convergence |\n",
    "\n",
    "In practice, backpropagation proceeds in the following manner for each training sample:\n",
    "\n",
    "1. Forward pass: Given the network input $a^0$, compute $a^m$ recursively by\n",
    " $$a^1 = \\sigma^1(W^1 a^0 + b^1), \\ldots, a^L = \\sigma^L(W^L a^{L - 1} + b^L)$$\n",
    "1. Backward pass: Compute \n",
    "$$\\frac{\\partial C}{\\partial z^L} = \\frac{\\partial C}{\\partial a^L} \\frac{\\partial a^L}{\\partial z^L}$$\n",
    "for the final layer based on the tables above, then recursively compute\n",
    "$$\\frac{\\partial C}{\\partial z^m} = \\left(W^{m + 1\\top} \\frac{\\partial C}{\\partial z^{m + 1}}\\right) \\circ \\frac{\\partial a^m}{\\partial z^m}$$\n",
    "for all other layers.  Plug these values into \n",
    "$$\\frac{\\partial C}{\\partial W^m} = \\frac{\\partial C}{\\partial z^m_i} a^{m - 1 \\top}$$\n",
    "and\n",
    "$$\\frac{\\partial C}{\\partial b^m} = \\frac{\\partial C}{\\partial z^m}$$\n",
    "to obtain the updates.\n",
    "\n",
    "### Example: Sigmoid network with cross-entropy loss using gradient descent\n",
    "\n",
    "A common network architecture is one with fully connected layers where each layer's nonlinearity is the sigmoid function $a^m = \\frac{1}{1 + e^{z^m}}$ and the cost function is the cross-entropy loss $(y - 1)\\log(1 - a^L) - y\\log(a^L)$.  To compute the updates for gradient descent, we first compute (based on the tables above)\n",
    "\\begin{align*}\n",
    "\\frac{\\partial C}{\\partial z^L} &= \\frac{\\partial C}{\\partial a^L} \\frac{\\partial a^L}{\\partial z^L}\\\\\n",
    "&= \\left(\\frac{a^L - y}{a^L(1 - a^L)}\\right)a^L(1 - a^L)\\\\\n",
    "&= a^L - y\n",
    "\\end{align*}\n",
    "From here, we can compute\n",
    "\\begin{align*}\n",
    "\\frac{\\partial C}{\\partial z^{L - 1}} &= \\left(W^{L\\top} \\frac{\\partial C}{\\partial z^L} \\right) \\circ \\frac{\\partial a^{L - 1}}{\\partial z^{L - 1}}\\\\\n",
    "&= W^{L\\top} (a^L - y) \\circ a^{L - 1}(1 - a^{L - 1})\\\\\n",
    "\\frac{\\partial C}{\\partial z^{L - 2}} &= \\left(W^{L - 1\\top} \\frac{\\partial C}{\\partial z^{L - 1}} \\right) \\circ \\frac{\\partial a^{L - 2}}{\\partial z^{L - 2}}\\\\\n",
    "&= W^{L - 1\\top} \\left(W^{L\\top} (a^L - y) \\circ a^{L - 1}(1 - a^{L - 1})\\right) \\circ a^{L - 2}(1 - a^{L - 2})\n",
    "\\end{align*}\n",
    "and so on, until we have computed $\\frac{\\partial C}{\\partial z^m}$ for $m \\in \\{1, \\ldots, L\\}$.  This allows us to compute  $\\frac{\\partial C}{\\partial W^m_{ij}}$ and $\\frac{\\partial C}{\\partial b^m_i}$, e.g.\n",
    "\\begin{align*}\n",
    "\\frac{\\partial C}{\\partial W^L} &= \\frac{\\partial C}{\\partial z^L} a^{L - 1 \\top}\\\\\n",
    "&= (a^L - y)a^{L - 1\\top}\\\\\n",
    "\\frac{\\partial C}{\\partial W^{L - 1}} &= \\frac{\\partial C}{\\partial z^{L - 1}} a^{L - 2 \\top}\\\\\n",
    "&= W^{L\\top} (a^L - y) \\circ a^{L - 1}(1 - a^{L - 1}) a^{L - 2\\top}\n",
    "\\end{align*}\n",
    "and so on.  Standard gradient descent then updates each parameter as follows:\n",
    "$$W^m = W^m - \\lambda \\frac{\\partial C}{\\partial W^m}$$\n",
    "$$b^m = b^m - \\lambda \\frac{\\partial C}{\\partial b^m}$$\n",
    "where $\\lambda$ is the learning rate.  This process is repeated until some stopping criteria is met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Python example\n",
    "\n",
    "Due to the recursive nature of the backpropagation algorithm, it lends itself well to software implementations.  The following code implements a multi-layer perceptron which is trained using backpropagation with user-supplied nonlinearities, layer sizes, and cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure python 3 forward compatibility\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "class SigmoidLayer:\n",
    "    def __init__(self, n_input, n_output):\n",
    "        self.W = np.random.randn(n_output, n_input)\n",
    "        self.b = np.random.randn(n_output, 1)\n",
    "    def output(self, X):\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "        return sigmoid(self.W.dot(X) + self.b)\n",
    "\n",
    "class SigmoidNetwork:\n",
    "\n",
    "    def __init__(self, layer_sizes):\n",
    "        '''\n",
    "        :parameters:\n",
    "            - layer_sizes : list of int\n",
    "                List of layer sizes of length L+1 (including the input dimensionality)\n",
    "        '''\n",
    "        self.layers = []\n",
    "        for n_input, n_output in zip(layer_sizes[:-1], layer_sizes[1:]):\n",
    "            self.layers.append(SigmoidLayer(n_input, n_output))\n",
    "    \n",
    "    def train(self, X, y, learning_rate=0.2):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(-1, 1)\n",
    "        if y.ndim == 1:\n",
    "            y = y.reshape(1, -1)\n",
    "        \n",
    "        # Forward pass - compute a^n for n in {0, ... L}\n",
    "        layer_outputs = [X]\n",
    "        for layer in self.layers:\n",
    "            layer_outputs.append(layer.output(layer_outputs[-1]))\n",
    "        \n",
    "        # Backward pass - compute \\partial C/\\partial z^m for m in {L, ..., 1}\n",
    "        cost_partials = [layer_outputs[-1] - y]\n",
    "        for layer, layer_output in zip(reversed(self.layers), reversed(layer_outputs[:-1])):\n",
    "            cost_partials.append(layer.W.T.dot(cost_partials[-1])*layer_output*(1 - layer_output))\n",
    "        cost_partials.reverse()\n",
    "        \n",
    "        # Compute weight gradient step\n",
    "        W_updates = []\n",
    "        for cost_partial, layer_output in zip(cost_partials[1:], layer_outputs[:-1]):\n",
    "            W_updates.append(cost_partial.dot(layer_output.T)/X.shape[1])\n",
    "        # and biases\n",
    "        b_updates = [cost_partial.mean(axis=1).reshape(-1, 1) for cost_partial in cost_partials[1:]]\n",
    "        \n",
    "        for W_update, b_update, layer in zip(W_updates, b_updates, self.layers):\n",
    "            layer.W -= W_update*learning_rate\n",
    "            layer.b -= b_update*learning_rate\n",
    "\n",
    "    def output(self, X):\n",
    "        a = np.array(X)\n",
    "        if a.ndim == 1:\n",
    "            a = a.reshape(-1, 1)\n",
    "        for layer in self.layers:\n",
    "            a = layer.output(a)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input\tOutput\tQuantized\n",
      "[0, 0]\t0.0134\t[0]\n",
      "[1, 0]\t0.9823\t[1]\n",
      "[0, 1]\t0.9875\t[1]\n",
      "[1, 1]\t0.0114\t[0]\n"
     ]
    }
   ],
   "source": [
    "nn = SigmoidNetwork([2, 2, 1])\n",
    "X = np.array([[0, 1, 0, 1], \n",
    "              [0, 0, 1, 1]])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "for n in range(int(1e3)):\n",
    "    nn.train(X, y, learning_rate=1.)\n",
    "print(\"Input\\tOutput\\tQuantized\")\n",
    "for i in [[0, 0], [1, 0], [0, 1], [1, 1]]:\n",
    "    print(\"{}\\t{:.4f}\\t{}\".format(i, nn.output(i)[0, 0], 1*(nn.output(i)[0] > .5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cbee5a58935421fb56dd437db9af560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=5, description='beta', max=25, min=-1), Output()), _dom_classes=('widget…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "logistic = lambda h, beta: 1./(1 + np.exp(-beta * h))\n",
    "\n",
    "@interact(beta=(-1, 25))\n",
    "def logistic_plot(beta=5):\n",
    "    hvals = np.linspace(-2, 2)\n",
    "    plt.plot(hvals, logistic(hvals, beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has the advantage of having a simple derivative:\n",
    "\n",
    "$$\\frac{dg}{dh} = \\beta g(h)(1 - g(h))$$\n",
    "\n",
    "Alternatively, the hyperbolic tangent function is also sigmoid:\n",
    "\n",
    "$$g(h) = \\tanh(h) = \\frac{\\exp(h) - \\exp(-h)}{\\exp(h) + \\exp(-h)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b774c9d54f24c70b769bf4a2e797d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=5, description='theta', max=25, min=-1), Output()), _dom_classes=('widge…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hyperbolic_tangent = lambda h: (np.exp(h) - np.exp(-h)) / (np.exp(h) + np.exp(-h))\n",
    "\n",
    "@interact(theta=(-1, 25))\n",
    "def tanh_plot(theta=5):\n",
    "    hvals = np.linspace(-2, 2)\n",
    "    h = hvals*theta\n",
    "    plt.plot(hvals, hyperbolic_tangent(h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent\n",
    "---\n",
    "The simplest algorithm for iterative minimization of differentiable functions is known as just **gradient descent**.\n",
    "Recall that the gradient of a function is defined as the vector of partial derivatives:\n",
    "\n",
    "$$\\nabla f(x) =  [{\\partial{f}{x_1}, \\partial{f}{x_2}, \\ldots, \\partial{f}{x_n}}]$$\n",
    "\n",
    "and that the gradient of a function always points towards the direction of maximal increase at that point.\n",
    "\n",
    "Equivalently, it points *away* from the direction of maximum decrease - thus, if we start at any point, and keep moving in the direction of the negative gradient, we will eventually reach a local minimum.\n",
    "\n",
    "This simple insight leads to the Gradient Descent algorithm. Outlined algorithmically, it looks like this:\n",
    "\n",
    "1. Pick a point $x_0$ as your initial guess.\n",
    "2. Compute the gradient at your current guess:\n",
    "$v_i = \\nabla f(x_i)$\n",
    "3. Move by $\\alpha$ (your step size) in the direction of that gradient:\n",
    "$x_{i+1} = x_i + \\alpha v_i$\n",
    "4. Repeat steps 1-3 until your function is close enough to zero (until $f(x_i) < \\varepsilon$ for some small tolerance $\\varepsilon$)\n",
    "\n",
    "Note that the step size, $\\alpha$, is simply a parameter of the algorithm and has to be fixed in advance. \n",
    "\n",
    "<img src='http://ludovicarnold.altervista.org/wp-content/uploads/2015/01/gradient-trajectory.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the hyperbolic tangent function asymptotes at -1 and 1, rather than 0 and 1, which is sometimes beneficial, and its derivative is simple:\n",
    "\n",
    "$$\\frac{d \\tanh(x)}{dx} = 1 - \\tanh^2(x)$$\n",
    "\n",
    "Performing gradient descent will allow us to change the weights in the direction that optimially reduces the error. The next trick will be to employ the **chain rule** to decompose how the error changes as a function of the input weights into the change in error as a function of changes in the inputs to the weights, mutliplied by the changes in input values as a function of changes in the weights. \n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w} = \\frac{\\partial E}{\\partial h}\\frac{\\partial h}{\\partial w}$$\n",
    "\n",
    "This will allow us to write a function describing the activations of the output weights as a function of the activations of the hidden layer nodes and the output weights, which will allow us to propagate error backwards through the network.\n",
    "\n",
    "The second term in the chain rule simplifies to:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial h_k}{\\partial w_{jk}} &= \\frac{\\partial \\sum_l w_{lk} a_l}{\\partial w_{jk}}  \\\\\n",
    "&= \\sum_l \\frac{\\partial w_{lk} a_l}{\\partial w_{jk}} \\\\\n",
    "& = a_j\n",
    "\\end{align}$$\n",
    "\n",
    "where $a_j$ is the activation of the jth hidden layer neuron.\n",
    "\n",
    "For the first term in the chain rule above, we decompose it as well:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial h_k} = \\frac{\\partial E}{\\partial y_k}\\frac{\\partial y_k}{\\partial h_k} = \\frac{\\partial E}{\\partial g(h_k)}\\frac{\\partial g(h_k)}{\\partial h_k}$$\n",
    "\n",
    "The second term of this chain rule is just the derivative of the activation function, which we have chosen to have a conveneint form, while the first term simplifies to:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial g(h_k)} = \\frac{\\partial}{\\partial g(h_k)}\\left[\\frac{1}{2} \\sum_k (t_k - y_k)^2 \\right] = t_k - y_k$$\n",
    "\n",
    "Combining these, and assuming (for illustration) a logistic activiation function, we have the gradient:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w} = (t_k - y_k) y_k (1-y_k) a_j$$\n",
    "\n",
    "Which ends up getting plugged into the weight update formula that we saw in the single-layer perceptron:\n",
    "\n",
    "$$w_{jk} \\leftarrow w_{jk} - \\eta (t_k - y_k) y_k (1-y_k) a_j$$\n",
    "\n",
    "Note that here we are *subtracting* the second term, rather than adding, since we are doing gradient descent.\n",
    "\n",
    "We can now outline the MLP learning algorithm:\n",
    "\n",
    "1. Initialize all $w_{jk}$ to small random values\n",
    "2. For each input vector, conduct forward propagation:\n",
    "    * compute activation of each neuron $j$ in hidden layer (here, sigmoid):\n",
    "    $$h_j = \\sum_i x_i v_{ij}$$\n",
    "    $$a_j = g(h_j) = \\frac{1}{1 + \\exp(-\\beta h_j)}$$\n",
    "    * when the output layer is reached, calculate outputs similarly:\n",
    "    $$h_k = \\sum_k a_j w_{jk}$$\n",
    "    $$y_k = g(h_k) = \\frac{1}{1 + \\exp(-\\beta h_k)}$$\n",
    "3. Calculate loss for resulting predictions:\n",
    "    * compute error at output:\n",
    "    $$\\delta_k = (t_k - y_k) y_k (1-y_k)$$\n",
    "4. Conduct backpropagation to get partial derivatives of cost with respect to weights, and use these to update weights:\n",
    "    * compute error of the hidden layers:\n",
    "    $$\\delta_{hj} = \\left[\\sum_k w_{jk} \\delta_k \\right] a_j(1-a_j)$$\n",
    "    * update output layer weights:\n",
    "    $$w_{jk} \\leftarrow w_{jk} - \\eta \\delta_k a_j$$\n",
    "    * update hidden layer weights:\n",
    "    $$v_{ij} \\leftarrow v_{ij} - \\eta \\delta_{hj} x_i$$\n",
    "    \n",
    "Return to (2) and iterate until learning completes. Best practice is to shuffle input vectors to avoid training in the same order.\n",
    "\n",
    "Its important to be aware that because gradient descent is a hill-climbing (or descending) algorithm, it is liable to be caught in local minima with respect to starting values. Therefore, it is worthwhile training several networks using a range of starting values for the weights, so that you have a better chance of discovering a globally-competitive solution.\n",
    "\n",
    "One useful performance enhancement for the MLP learning algorithm is the addition of **momentum** to the weight updates. This is just a coefficient on the previous weight update that increases the correlation between the current weight and the weight after the next update. This is particularly useful for complex models, where falling into local mimima is an issue; adding momentum will give some weight to the previous direction, making the resulting weights essentially a weighted average of the two directions. Adding momentum, along with a smaller learning rate, usually results in a more stable algorithm with quicker convergence. When we use momentum, we lose this guarantee, but this is generally seen as a small price to pay for the improvement momentum usually gives.\n",
    "\n",
    "A weight update with momentum looks like this:\n",
    "\n",
    "$$w_{jk} \\leftarrow w_{jk} - \\eta \\delta_k a_j + \\alpha \\Delta w_{jk}^{t-1}$$\n",
    "\n",
    "where $\\alpha$ is the momentum (regularization) parameter and $\\Delta w_{jk}^{t-1}$ the update from the previous iteration.\n",
    "\n",
    "The multi-layer pereptron is implemented below in the `MLP` class. The implementation uses the scikit-learn interface, so it is uses in the same way as other supervised learning algorithms in that package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "       #############   ###########  Neural Networks diagram and Training   ###########   ############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     ###########    ############    Diagram of a Neural Network    #############    ###############\n",
    " <img src='images/nn.jpg'>\n",
    "\n",
    "   There are 3 different types of layers lies inside a Neural Networks.\n",
    "    \n",
    "         * Input Layer    *  Hidden Layers    *  Output Layer\n",
    "    \n",
    "   In Input Layer, Number of Perceptrons depends upon the total number of Features available in dataset.\n",
    "    \n",
    "   Perceptrons of Input Layer receives the inputted data. In above case, Row 1 is going to be inputted in Input Layer,\n",
    "       then 2nd Row, 3rd Row and so on .\n",
    "    \n",
    "   The main function of Input layer is just only getting the inputs from dataset.\n",
    "\n",
    "   This Inputted data goes to 2nd Layer of Neural networks (Hidden layer) in such a way that Data from each Perceptron\n",
    "       of Input layer goes inputted to each and every Perceptrons of Hidden layer.\n",
    "\n",
    "   When we try to pass our data from Input layer to Hidden layer, We multiply some randomly selected weights, W1 to inputs X1 \n",
    "       and then add some randomly selected Bias, B1 to the inputs. \n",
    "     Then this Linear mix of inputs (W1.X1 + B1) get entered into each Perceptrons of Hidden layer.\n",
    "        \n",
    "   Weights and Bias are learnable parameters and they are added to get patterens by making relations in dataset more easily.\n",
    "\n",
    "   If we have only 2 Layers i.e. Input Layer and Output Layer, Whatever data we passes to Input Layer will be outputted \n",
    "       based on Output Functions and with no parameters it is very difficult to find a relationship. \n",
    "\n",
    "      ##############   #############   Assigning Weights and Bias To Inputs   ##############   ###############\n",
    "<img src='images/weight.png'>\n",
    "\n",
    " In above Neural network, We have assigned different-different Weights and Bias to inputs.\n",
    "    \n",
    "    \n",
    "    In Hidden layer, Each Perceptron is getting a Linear Mix of inputs from all perceptrons of Input layer. \n",
    "       so The summation of all Linear mix of inputs is considered to be Inputted to a single Perceptron. \n",
    "\n",
    "    ##############    ##############   Input of Perceptrons in Hidden layer   ##############    #############\n",
    "<img src='images/total-input.jpg'>\n",
    "    \n",
    "    \n",
    " Inside a Hidden layer, a Linear mix of data enters as Input, so we need some particular function which will be able to \n",
    "    Filter, restrict, normalize and Non-Linearize the Inputted dataset, which is passed from Input layer to Hidden layer.\n",
    "      ThereFore activation Function comes into picture.\n",
    "        \n",
    " Activation functions are kind of functions, which always helps neurons to fire itself in certain direction or\n",
    "    in a certain range and also try to restricts some sort of dataset.\n",
    "    \n",
    " Examples of Activation function - \n",
    "\n",
    "        *  Sigmoid Activation Function         *  Tanh Activation Function  \n",
    "        *  RelU Activation Function            *  Leaky RelU Activation function etc.\n",
    "        \n",
    "  We can use a Single Activation Function inside a Hidden Layer.\n",
    " Activation function does some particular operations and give some outputs (say O1, O2), which get multiplied with some different\n",
    "   weights again and then enters into Next Layer. Next layer may be a Hidden Layer or Output layer.\n",
    "    \n",
    " We can add as many Hidden layers but in a certain limit.\n",
    "\n",
    " In Output Layer, We again uses a Function which is known as Output Function.\n",
    "    Here all Inputs are get added together by using Output function and then we get a final output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Function\n",
    "\n",
    "Activation functions helps to determine the output of a neural network. These type of functions are attached to each neuron in the network, and determines whether it should be activated or not, based on whether each neuron’s input is relevant for the model’s prediction. \n",
    "\n",
    "> Activation function also helps to normalize the output of each neuron to a range between 1 and 0 or between -1 and 1.\n",
    "\n",
    "\n",
    "<img src='images/1.jpg'>\n",
    "\n",
    "In a neural network, inputs are fed into the neurons in the input layer. Each neuron has a weight, and multiplying the input number with the weight gives the output of the neuron, which is transferred to the next layer.\n",
    "\n",
    "The activation function is a mathematical “gate” in between the input feeding the current neuron and its output going to the next layer. It can be as simple as a step function that turns the neuron output on and off, depending on a rule or threshold.\n",
    "\n",
    "\n",
    "> Neural networks use non-linear activation functions, which can help the network learn complex data, compute and learn almost any function representing a question, and provide accurate predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commonly used activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1.   Sigmod function**\n",
    "\n",
    "\n",
    "\n",
    "The function formula and chart are as follows\n",
    "\n",
    "![alt](images/sig.svg)\n",
    "\n",
    "![alt](img/2.png)\n",
    "\n",
    "The Sigmoid function is the most frequently used activation function in the beginning of deep learning. It is a smoothing function that is easy to derive.\n",
    "\n",
    "In the sigmoid function, we can see that its output is in the open interval (0,1). We can think of probability, but in the strict sense, don't treat it as probability. The sigmoid function was once more popular. It can be thought of as the firing rate of a neuron. In the middle where the slope is relatively large, it is the sensitive area of the neuron. On the sides where the slope is very gentle, it is the neuron's inhibitory area.\n",
    "\n",
    "The function itself has certain defects.\n",
    "\n",
    "1) When the input is slightly away from the coordinate origin, the gradient of the function becomes very small, almost zero. In the process of neural network backpropagation, we all use the chain rule of differential to calculate the differential of each weight w. When the backpropagation passes through the sigmod function, the differential on this chain is very small. Moreover, it may pass through many sigmod functions, which will eventually cause the weight w to have little effect on the loss function, which is not conducive to the optimization of the weight. This The problem is called gradient saturation or gradient dispersion.\n",
    "\n",
    "2) The function output is not centered on 0, which will reduce the efficiency of weight update.\n",
    "\n",
    "3) The sigmod function performs exponential operations, which is slower for computers.\n",
    "\n",
    "\n",
    "Advantages of Sigmoid Function : -\n",
    "\n",
    "1. Smooth gradient, preventing “jumps” in output values.\n",
    "2. Output values bound between 0 and 1, normalizing the output of each neuron.\n",
    "3. Clear predictions, i.e very close to 1 or 0.\n",
    "\n",
    "\n",
    "Sigmoid has three major disadvantages:\n",
    "* Prone to gradient vanishing\n",
    "* Function output is not zero-centered\n",
    "* Power operations are relatively time consuming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.  tanh function**\n",
    "\n",
    "The tanh function formula and curve are as follows\n",
    "\n",
    "![alt](img/tan.svg)\n",
    "![alt](img/3.png)\n",
    "\n",
    "Tanh is a hyperbolic tangent function. The curves of tanh function and sigmod function are relatively similar. Let ’s compare them. First of all, when the input is large or small, the output is almost smooth and the gradient is small, which is not conducive to weight update. The difference is the output interval. \n",
    "\n",
    "The output interval of tanh is 1), and the whole function is 0-centric, which is better than sigmod.\n",
    "\n",
    "In general binary classification problems, the tanh function is used for the hidden layer and the sigmod function is used for the output layer. However, these are not static, and the specific activation function to be used must be analyzed according to the specific problem, or it depends on debugging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.  ReLU function**\n",
    "\n",
    "ReLU function formula and curve are as follows\n",
    "\n",
    "![alt](img/relu.svg)\n",
    "![alt](img/relu.png)\n",
    "\n",
    "The ReLU function is actually a function that takes the maximum value. Note that this is not fully interval-derivable, but we can take sub-gradient, as shown in the figure above. Although ReLU is simple, it is an important achievement in recent years.\n",
    "\n",
    "The ReLU (Rectified Linear Unit) function is an activation function that is currently more popular. Compared with the sigmod function and the tanh function, it has the following advantages:\n",
    "\n",
    "1) When the input is positive, there is no gradient saturation problem.\n",
    "\n",
    "2) The calculation speed is much faster. The ReLU function has only a linear relationship. Whether it is forward or backward, it is much faster than sigmod and tanh. (Sigmod and tanh need to calculate the exponent, which will be slower.)\n",
    "\n",
    "Ofcourse, there are disadvantages:\n",
    "\n",
    "1) When the input is negative, ReLU is completely inactive, which means that once a negative number is entered, ReLU will die. In this way, in the forward propagation process, it is not a problem. Some areas are sensitive and some are insensitive. But in the backpropagation process, if you enter a negative number, the gradient will be completely zero, which has the same problem as the sigmod function and tanh function.\n",
    "\n",
    "2) We find that the output of the ReLU function is either 0 or a positive number, which means that the ReLU function is not a 0-centric function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Leaky ReLU function\n",
    "\n",
    "![alt](img/lrelu.svg)\n",
    "![alt](img/lrelu.png)\n",
    "\n",
    "In order to solve the Dead ReLU Problem, people proposed to set the first half of ReLU 0.01x instead of 0. Another intuitive idea is a parameter-based method, Parametric ReLU : f(x)= max(alpha x,x), which alpha can be learned from back propagation. In theory, Leaky ReLU has all the advantages of ReLU, plus there will be no problems with Dead ReLU, but in actual operation, it has not been fully proved that Leaky ReLU is always better than ReLU.\n",
    "\n",
    "### 5. ELU (Exponential Linear Units) function\n",
    "\n",
    "![alt](img/elu.svg)\n",
    "![alt](img/elu.jpg)\n",
    "\n",
    "ELU is also proposed to solve the problems of ReLU. Obviously, ELU has all the advantages of ReLU, and:\n",
    "\n",
    "* No Dead ReLU issues\n",
    "* The mean of the output is close to 0, zero-centered\n",
    "\n",
    "One small problem is that it is slightly more computationally intensive. Similar to Leaky ReLU, although theoretically better than ReLU, there is currently no good evidence in practice that ELU is always better than ReLU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.  Softmax\n",
    "\n",
    "![alt](img/soft.png)\n",
    "\n",
    "\n",
    "for an arbitrary real vector of length K, Softmax can compress it into a real vector of length K with a value in the range (0, 1), and the sum of the elements in the vector is 1. \n",
    "\n",
    "It also has many applications in Multiclass Classification and neural networks. Softmax is different from the normal max function: the max function only outputs the largest value, and Softmax ensures that smaller values have a smaller probability and will not be discarded directly. It is a \"max\" that is \"soft\".\n",
    "\n",
    "The denominator of the Softmax function combines all factors of the original output value, which means that the different probabilities obtained by the Softmax function are related to each other.\n",
    "In the case of binary classification, for Sigmoid, there are:\n",
    "\n",
    "![alt](img/soft1.png)\n",
    "\n",
    "For Softmax with K = 2, there are:\n",
    "\n",
    "![alt](img/soft2.png)\n",
    "\n",
    "\n",
    "Among them: It\n",
    "\n",
    "![alt](img/soft4.png)\n",
    "\n",
    "can be seen that in the case of binary classification, Softmax is degraded to Sigmoid.\n",
    "\n",
    "![alt](img/soft31.png)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.  PRelu (Parametric ReLU)\n",
    "\n",
    "![alt](img/CAPTURE.png)\n",
    "\n",
    "PReLU is also an improved version of ReLU. In the negative region, PReLU has a small slope, which can also avoid the problem of ReLU death. Compared to ELU, PReLU is a linear operation in the negative region. Although the slope is small, it does not tend to 0, which is a certain advantage.\n",
    "\n",
    "![alt](img/CAPTURE1.png)\n",
    "\n",
    "We look at the formula of PReLU. The parameter α is generally a number between 0 and 1, and it is generally relatively small, such as a few zeros. When α = 0.01, we call PReLU as Leaky Relu , it is regarded as a special case PReLU it.\n",
    "\n",
    "Above, yᵢ is any input on the ith channel and aᵢ is the negative slope which is a learnable parameter.\n",
    "* if aᵢ=0, f becomes ReLU\n",
    "* if aᵢ>0, f becomes leaky ReLU\n",
    "* if aᵢ is a learnable parameter, f becomes PReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Swish (A Self-Gated) Function\n",
    "\n",
    "![alt](img/swish.png)\n",
    "\n",
    "The formula is: **y = x * sigmoid (x)**\n",
    "\n",
    "Swish's design was inspired by the use of sigmoid functions for gating in LSTMs and highway networks. We use the same value for gating to simplify the gating mechanism, which is called **self-gating**. \n",
    "\n",
    "The advantage of self-gating is that it only requires a simple scalar input, while normal gating requires multiple scalar inputs. This feature enables self-gated activation functions such as Swish to easily replace activation functions that take a single scalar as input (such as ReLU) without changing the hidden capacity or number of parameters.\n",
    "\n",
    "1) Unboundedness (unboundedness) is helpful to prevent gradient from gradually approaching 0 during slow training, causing saturation. At the same time, being bounded has advantages, because bounded active functions can have strong reguairzation, and larger negative inputs will be resolved.\n",
    "\n",
    "2) At the same time, smoothness also plays an important role in optimization and generalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.  Maxout\n",
    "\n",
    "The Maxout activation function is defined as follows\n",
    "\n",
    "![alt](img/max.jpeg)\n",
    "\n",
    "One relatively popular choice is the Maxout neuron (introduced recently by Goodfellow et al.) that generalizes the ReLU and its leaky version. Notice that both ReLU and Leaky ReLU are a special case of this form (for example, for ReLU we have w1,b1 =0).The Maxout neuron therefore enjoys all the benefits of a ReLU unit (linear regime of operation, no saturation) and does not have its drawbacks\n",
    "\n",
    "The Maxout activation is a generalization of the ReLU and the leaky ReLU functions. It is a learnable activation function.\n",
    "\n",
    "Maxout can be seen as adding a layer of activation function to the deep learning network, which contains a parameter k. Compared with ReLU, sigmoid, etc., this layer is special in that it adds k neurons and then outputs the largest activation value. value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.  Softplus\n",
    "\n",
    "![alt](img/softplus.png)\n",
    "\n",
    "The softplus function is similar to the ReLU function, but it is relatively smooth.It is unilateral suppression like ReLU.It has a wide acceptance range (0, + inf).\n",
    "\n",
    "Softplus function: **f(x) = ln(1+exp x)**\n",
    "\n",
    "\n",
    "\n",
    "## **------------------------------------------------------NOTE--------------------------------------------------------------**\n",
    "\n",
    "## **Generally speaking, these activation functions have their own advantages and disadvantages. There is no statement that indicates which ones are not working, and which activation functions are good. All the good and bad must be obtained by experiments.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Activation functions are used in Hidden layers and Output functions are used in Output Layer and both are similar Functions.\n",
    "\n",
    " With Predicted Output and Expected output we gets a Loss, because we have selected some Weights and Bias randomly.\n",
    "\n",
    " Then we perform a specific operation in which we try to update Weights and Bias based on losses and this operation is known as \n",
    "      Back Propagation.\n",
    "        \n",
    " The process of passing a dataset from Input Layer to Output Layer is known as Feed Forward Connection or\n",
    "    Feed Forward Network (FFN) or Forward pass. In FFN, it never try to learn anything.\n",
    "        \n",
    " In Back Propagation, we uses an Optimiser to calculate new values of Weights and Bias.\n",
    " This Back Propagation always starts from Output layer and goes towards Input layer and updates all parameters.\n",
    "    \n",
    " After Back Propagation, Feed Forward Passing starts again with updated parameters and we gets a Loss again.\n",
    "   Then we repeat back Propagation and uses Optimiser again and then FFN starts..  \n",
    "     This keeps repeating continuously, unless and untill we get a very minimal Loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions\n",
    "\n",
    "## 1. L1 and L2 loss\n",
    "\n",
    "*L1* and *L2* are two common loss functions in machine learning which are mainly used to minimize the error.\n",
    "\n",
    "**L1 loss function** are also known as **Least Absolute Deviations** in short **LAD**.\n",
    "**L2 loss function** are also known as **Least square errors** in short **LS**.\n",
    "\n",
    "Let's get brief of these two\n",
    "\n",
    "### L1 Loss function\n",
    "It is used to minimize the error which is the sum of all the absolute differences in between the true value and the predicted value.\n",
    "\n",
    "<img src=\".\\Images\\img13.png\">\n",
    "\n",
    "### L2 Loss Function\n",
    "It is also used to minimize the error which is the sum of all the squared differences in between the true value and the pedicted value.\n",
    "\n",
    "<img src=\".\\Images\\img15.png\">\n",
    "\n",
    "**The disadvantage** of the **L2 norm** is that when there are outliers, these points will account for the main component of the loss. For example, the true value is 1, the prediction is 10 times, the prediction value is 1000 once, and the prediction value of the other times is about 1, obviously the loss value is mainly dominated by 1000.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_guess = tf.lin_space(-1., 1., 100)\n",
    "x_actual = tf.constant(0,dtype=tf.float32)\n",
    "\n",
    "l1_loss = tf.abs((x_guess-x_actual))\n",
    "l2_loss = tf.square((x_guess-x_actual))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    x_,l1_,l2_ = sess.run([x_guess, l1_loss, l2_loss])\n",
    "    plt.plot(x_,l1_,label='l1_loss')\n",
    "    plt.plot(x_,l2_,label='l2_loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Huber Loss \n",
    "\n",
    "Huber Loss is often used in regression problems. Compared with L2 loss, Huber Loss is less sensitive to outliers(because if the residual is too large, it is a piecewise function, loss is a linear function of the residual).\n",
    "\n",
    "<img src=\".\\Images\\img1.png\">\n",
    "\n",
    "Among them, $\\delta$ is a set parameter, $y$ represents the real value, and $f(x)$ represents the predicted value.\n",
    "\n",
    "The advantage of this is that when the residual is small, the loss function is L2 norm, and when the residual is large, it is a linear function of L1 norm\n",
    "\n",
    "### Pseudo-Huber loss function \n",
    "\n",
    "A smooth approximation of Huber loss to ensure that each order is differentiable.\n",
    "\n",
    "<img src=\".\\Images\\img2.png\">\n",
    "\n",
    "Where $\\delta$ is the set parameter, the larger the value, the steeper the linear part on both sides.\n",
    "\n",
    "<img src=\".\\Images\\img3.png\">\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_guess2 = tf.linspace(-3.,5.,500)\n",
    "x_actual2 = tf.convert_to_tensor([1.]*500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3.Hinge Loss\n",
    "\n",
    "Hinge loss is often used for binary classification problems, such as ground true: t = 1 or -1, predicted value y = wx + b\n",
    "\n",
    "In the svm classifier, the definition of hinge loss is\n",
    "\n",
    "<img src=\".\\Images\\img4.png\">\n",
    "\n",
    "In other words, the closer the y is to t, the smaller the loss will be.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Hinge loss\n",
    "#hinge_loss = tf.losses.hinge_loss(labels=x_actual2, logits=x_guess2)\n",
    "hinge_loss = tf.maximum(0.,1.-(x_guess2*x_actual2))\n",
    "0with tf.Session() as sess:\n",
    "    x_,hin_ = sess.run([x_guess2, hinge_loss])\n",
    "    plt.plot(x_,hin_,'--', label='hin_')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Cross-entropy loss\n",
    "\n",
    "<img src=\".\\Images\\img7.png\">\n",
    "\n",
    "The above is mainly to say that cross-entropy loss is mainly applied to binary classification problems. The predicted value is a probability value and the loss is defined according to the cross entropy. Note the value range of the above value: the predicted value of y should be a probability and the value range is [0,1]\n",
    "\n",
    "<img src=\".\\Images\\img8.png\">\n",
    "\n",
    "## 5.Sigmoid-Cross-entropy loss\n",
    "\n",
    "The above cross-entropy loss requires that the predicted value is a probability. Generally, we calculate $scores = x*w + b$. Entering this value into the sigmoid function can compress the value range to (0,1).\n",
    "\n",
    "<img src=\".\\Images\\img9.png\">\n",
    "\n",
    "It can be seen that the sigmoid function smoothes the predicted value(such as directly inputting 0.1 and 0.01 and inputting 0.1, 0.01 sigmoid and then entering, the latter will obviously have a much smaller change value), which makes the predicted value of sigmoid-ce far from the label loss growth is not so steep.\n",
    "\n",
    "## 6.Softmax cross-entropy loss\n",
    "\n",
    "First, the softmax function can convert a set of fraction vectors into corresponding probability vectors. Here is the definition of softmax function\n",
    "\n",
    "<img src=\".\\Images\\img10.png\">\n",
    "\n",
    "As above, softmax also implements a vector of 'squashes' k-dimensional real value to the [0,1] range of k-dimensional, while ensuring that the cumulative sum is 1.\n",
    "\n",
    "According to the definition of cross entropy, probability is required as input.Sigmoid-cross-entropy-loss uses sigmoid to convert the score vector into a probability vector, and softmax-cross-entropy-loss uses a softmax function to convert the score vector into a probability vector.\n",
    "\n",
    "According to the definition of cross entropy loss.\n",
    "\n",
    "<img src=\".\\Images\\img11.png\">\n",
    "\n",
    "where $p(x)$ represents the probability that classification $x$ is a correct classification, and the value of $p$ can only be 0 or 1. This is the prior value\n",
    "\n",
    "$q(x)$ is the prediction probability that the $x$ category is a correct classification, and the value range is (0,1)\n",
    "\n",
    "So specific to a classification problem with a total of C types, then $p(x_j)$, $(0 <_{=} j <_{=} C)$ must be only 1 and C-1 is 0(because there can be only one correct classification, correct the probability of classification as correct classification is 1, and the probability of the remaining classification as correct classification is 0)\n",
    "\n",
    "Then the definition of softmax-cross-entropy-loss can be derived naturally.\n",
    "\n",
    "Here is the definition of softmax-cross-entropy-loss.\n",
    "\n",
    "<img src=\".\\Images\\img12.png\">\n",
    "\n",
    "Where $f_j$ is the score of all possible categories, and $f_{y_i}$ is the score of ground true class\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch gradient descent\n",
    "\n",
    "**Gradient update rule:**\n",
    "BGD uses the data of the entire training set to calculate the gradient of the cost function to the parameters:\n",
    "\n",
    " \n",
    " **Disadvantages:**\n",
    "\n",
    "Because this method calculates **the gradient for the entire data set in one update, the calculation is very slow**, it will be very tricky to encounter a large number of data sets, and you cannot invest in new data to update the model in real time.\n",
    "\n",
    "We will define an iteration number epoch in advance, first calculate the gradient vector params_grad, and then update the parameter params along the direction of the gradient. The learning rate determines how big we take each step.\n",
    "\n",
    "**Batch gradient descent can converge to a global minimum for convex functions and to a local minimum for non-convex functions.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch gradient descent\n",
    "\n",
    "**Gradient update rule:**\n",
    "\n",
    "MBGD uses a small batch of samples, that is, n samples to calculate each time. In\n",
    "this way, it can reduce the variance when the parameters are updated, and the convergence is more stable.\n",
    " It can make full use of the highly optimized matrix operations in the deep learning library for more efficient gradient calculations.\n",
    "\n",
    " \n",
    "**The difference from SGD is that each cycle does not act on each sample, but a batch with n samples.**\n",
    "\n",
    ">Setting value of hyper-parameters:\n",
    "n Generally value is 50 ～ 256\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "* Mini-batch gradient descent does not guarantee good convergence,\n",
    "\n",
    "* If the learning rate is too small, the convergence rate will be slow. If it is too large, the loss function will oscillate or even deviate at the minimum value.\n",
    "One measure is to set a **larger learning rate**. When the change between two iterations is lower than a certain threshold, the learning rate is reduced.\n",
    "\n",
    "However, the setting of this threshold needs to be written in advance adapt to the characteristics of the data set.\n",
    "\n",
    "In addition, this method is to apply the **same learning rate** to all parameter updates. If our data is sparse, we would prefer to update the features with lower frequency.\n",
    "\n",
    "In addition, for **non-convex functions**, it is also necessary to avoid trapping at the local minimum or saddle point, because the error around the saddle point is the same, the gradients of all dimensions are close to 0, and SGD is easily trapped here.\n",
    "\n",
    "**Saddle points** are the curves, surfaces, or hypersurfaces of a saddle point neighborhood of a smooth function are located on different sides of a tangent to this point.\n",
    "For example, this two-dimensional figure looks like a saddle: it curves up in the x-axis direction and down in the y-axis direction, and the saddle point is (0,0).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-batching\n",
    "\n",
    "\n",
    "Mini-batching is a technique for training on subsets of the dataset instead of all the data at one time. This provides the ability to train a model, even if a computer lacks the memory to store the entire dataset.\n",
    "\n",
    "Mini-batching is computationally inefficient, since you can't calculate the loss simultaneously across all samples. However, this is a small price to pay in order to be able to run the model at all.\n",
    "\n",
    "It's also quite useful combined with SGD. The idea is to randomly shuffle the data at the start of each epoch, then create the mini-batches. For each mini-batch, you train the network weights with gradient descent. Since these batches are random, you're performing SGD with each batch.\n",
    "\n",
    "\n",
    "\n",
    "## Epochs\n",
    "\n",
    "An epoch is a single forward and backward pass of the whole dataset. This is used to increase the accuracy of the model without requiring more data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD (Stochastic gradient descent)\n",
    "\n",
    "\n",
    "\n",
    "**Gradient update rule:**\n",
    "Compared with BGD's calculation of gradients with all data at one time, SGD updates the gradient of each sample with each update.\n",
    "\n",
    "`x += - learning_rate * dx`\n",
    "\n",
    "where x is a parameter, dx is the gradient and learning rate is constant\n",
    "\n",
    "\n",
    "For large data sets, there may be similar samples, so BGD calculates the gradient. **There will be redundancy,\n",
    "and SGD is updated only once, there is no redundancy, it is faster, and new samples can be added.**\n",
    "\n",
    " ![alt](imgo/sgd.png)\n",
    " \n",
    " **<center>Figure :- Fluctuations in SGD</center>** \n",
    "\n",
    "**Disadvantages:**\n",
    "However, because SGD is updated more frequently, the cost function will have severe oscillations.\n",
    "BGD can converge to a local minimum, of course, the oscillation of SGD may jump to a better local minimum.\n",
    "\n",
    "When we decrease the learning rate slightly, the convergence of SGD and BGD is the same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum\n",
    "\n",
    "One disadvantage of the SGD method is that its update direction depends entirely on the current batch, so its update is very unstable. A simple way to solve this problem is to introduce momentum.\n",
    "\n",
    "**Momentum is momentum**, which simulates the inertia of an object when it is moving, that is, the direction of the previous update is retained to a certain extent during the update, while the current update gradient is used to fine-tune the final update direction. In this way, you can increase the stability to a certain extent, so that you can learn faster, and also have the ability to get rid of local optimization.\n",
    "\n",
    " ![alt](imgo/sgd1.png)\n",
    " \n",
    " **<center>Figure :- SGD without Momentum &&&  SGD without Momentum</center>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Adagrad\n",
    "\n",
    "Adagrad is an algorithm for gradient-based optimization which adapts the learning rate to the parameters, using low learning rates for parameters associated with frequently occurring features, and using high learning rates for parameters associated with infrequent features. \n",
    "\n",
    "So, it is well-suited for dealing with sparse data.\n",
    "\n",
    "But the same update rate may not be suitable for all parameters. For example, some parameters may have reached the stage where only fine-tuning is needed, but some parameters need to be adjusted a lot due to the small number of corresponding samples.\n",
    "\n",
    "Adagrad proposed this problem, an algorithm that adaptively assigns different learning rates to various parameters among them. The implication is that for each parameter, as its total distance updated increases, its learning rate also slows.\n",
    "\n",
    ">**GloVe word embedding uses adagrad where infrequent words required a greater update and frequent words require smaller updates.**\n",
    "\n",
    ">**Adagrad eliminates the need to manually tune the learning rate.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adadelta\n",
    "\n",
    "There are three problems with the Adagrad algorithm\n",
    "\n",
    "* The learning rate is monotonically decreasing.\n",
    "* The learning rate in the late training period is very small.\n",
    "* It requires manually setting a global initial learning rate.\n",
    "\n",
    ">**Adadelta is an extension of Adagrad and it also tries to reduce Adagrad’s aggressive, monotonically reducing the learning rate.**\n",
    "\n",
    ">It does this by restricting the window of the past accumulated gradient to some fixed size of w. Running average at time t then depends on the previous average and the current gradient.\n",
    "\n",
    ">In Adadelta we do not need to set the default learning rate as we take the ratio of the running average of the previous time steps to the current gradient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp\n",
    "\n",
    "The full name of RMSProp algorithm is called **Root Mean Square Prop**, which is an adaptive learning rate optimization algorithm proposed by Geoff Hinton. \n",
    "\n",
    "\n",
    ">RMSProp tries to resolve Adagrad’s radically diminishing learning rates by using a moving average of the squared gradient. It utilizes the magnitude of the recent gradient descents to normalize the gradient.\n",
    "\n",
    "\n",
    "Adagrad will accumulate all previous gradient squares, and RMSprop just calculates the corresponding average value, so it can alleviate the problem that the learning rate of the Adagrad algorithm drops quickly.\n",
    "\n",
    "The difference is that RMSProp calculates the **differential squared weighted average of the gradient** . This method is beneficial to eliminate the direction of large swing amplitude, and is used to correct the swing amplitude, so that the swing amplitude in each dimension is smaller. On the other hand, it also makes the network function converge faster. \n",
    "\n",
    "\n",
    ">In RMSProp learning rate gets adjusted automatically and it chooses a different learning rate for each parameter.\n",
    "\n",
    ">RMSProp divides the learning rate by the average of the exponential decay of squared gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam\n",
    "\n",
    "**Adaptive Moment Estimation (Adam)** is another method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients like Adadelta and RMSprop.\n",
    "\n",
    ">Adam also keeps an exponentially decaying average of past gradients, similar to momentum.\n",
    "\n",
    ">Adam can be viewed as a combination of Adagrad and RMSprop,(Adagrad) which works well on sparse gradients and (RMSProp) which works well in online and nonstationary settings repectively.\n",
    "\n",
    ">Adam implements the **exponential moving average of the gradients** to scale the learning rate instead of a simple average as in Adagrad. It keeps an exponentially decaying average of past gradients.\n",
    "\n",
    ">Adam is computationally efficient and has very less memory requirement.\n",
    "\n",
    ">Adam optimizer is one of the most popular and famous gradient descent optimization algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparisions\n",
    "\n",
    "![alt](imgo/comp.gif)\n",
    "\n",
    "**<center>Figure :- SGD optimization on loss surface contours</center>**\n",
    "\n",
    "![alt](imgo/sadd.gif)\n",
    "\n",
    "**<center>Figure :- SGD optimization on saddle point</center>**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to choose optimizers?\n",
    "\n",
    "- If the data is sparse, use the self-applicable methods, namely Adagrad, Adadelta, RMSprop, Adam.\n",
    "\n",
    "- RMSprop, Adadelta, Adam have similar effects in many cases.\n",
    "\n",
    "- Adam just added bias-correction and momentum on the basis of RMSprop,\n",
    "\n",
    "- As the gradient becomes sparse, Adam will perform better than RMSprop.\n",
    "\n",
    "**Overall, Adam is the best choice.**\n",
    "\n",
    ">SGD is used in many papers, without momentum, etc. Although SGD can reach a minimum value, it takes longer than other algorithms and may be trapped in the saddle point.\n",
    "\n",
    "- If faster convergence is needed, or deeper and more complex neural networks are trained, an adaptive algorithm is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
