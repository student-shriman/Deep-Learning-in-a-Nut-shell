{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# What is Deep Learning ??\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To understand what Deep Learning is, we first need to understand the relationship that Deep learning has with Machine learning,\n",
    "     Neural Networks, and Artificial Intelligence.\n",
    " \n",
    " The best way to think of this relationship is to visualize them as concentric circles.\n",
    "\n",
    "<img src='images/ai.png' width='300px'>\n",
    "\n",
    "\n",
    "  At the outer most ring you have Artificial Intelligence.\n",
    "    Second layer inside of Artificial Intelligence is Machine learning and Deep learning is inside the Machine learning at the centre.\n",
    "    \n",
    "    Broadly speaking, Deep Learning is a more approachable name for an Artificial Neural Network. \n",
    "    The “Deep” in Deep Learning refers to the depth of the network. An Artificial Neural Network can be very shallow.\n",
    "\n",
    "Neural networks are inspired by the structure of the cerebral cortex.\n",
    "     At the basic level is the perceptron, the mathematical representation of a biological neuron. \n",
    "         Like in the cerebral cortex, there can be several layers of interconnected perceptrons.\n",
    "\n",
    "\n",
    " The first layer is the input layer. Each node in this layer takes an input, and then passes its output as the input\n",
    "        to each node in the next layer. There are generally no connections between nodes in the same layer and \n",
    "             the last layer produces the outputs.\n",
    "\n",
    "\n",
    " We call the middle part the hidden layer. These neurons have no connection to the outside (e.g. input or output) and \n",
    "   are only activated by nodes in the previous layer.\n",
    "\n",
    "<img src='images/nn-diagram.png' width='540 px'>\n",
    "\n",
    "Think of Deep Learning as the technique for learning in Neural networks that utilizes multiple layers of abstraction to solve pattern recognition problems. In the 1980s, most neural networks were a single layer due to the cost of computation and availability of data.\n",
    "\n",
    "Machine learning is considered a branch or approach of Artificial intelligence, whereas Deep Learning is a specialized type of Machine Learning.\n",
    "\n",
    "\n",
    "Machine learning involves Computer's intelligence that doesn’t know the answers up front. Instead, the program will run against training data, verify the success of its attempts, and modify its approach accordingly.\n",
    "   \n",
    "Machine learning typical requires a sophisticated education, spanning software engineering and computer science\n",
    "     to statistical methods and linear algebra.\n",
    "\n",
    "\n",
    "There are two broad classes of machine learning methods:\n",
    "\n",
    "   * Supervised learning\n",
    "   * Unsupervised learning\n",
    "\n",
    "\n",
    "In supervised learning, a machine learning algorithm uses a labeled dataset to infer the desired outcome. This takes a lot of data and time, since the data needs to be labeled by hand. Supervised learning is great for classification and regression problems.\n",
    "\n",
    "For example, let’s say that we were running a company and want to determine the effect of bonuses on employee retention. If we had historical data – i.e. employee bonus amount and tenure – we could use supervised machine learning.\n",
    "\n",
    "With unsupervised learning, there aren’t any predefined or corresponding answers. The goal is to figure out the hidden patterns in the data. It’s usually used for clustering and associative tasks, like grouping customers by behavior. Amazon’s “customers who also bought…” recommendations are a type of associative task.\n",
    "\n",
    "While supervised learning can be useful, we often have to resort to unsupervised learning. Deep learning has proven to be an effective unsupervised learning technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is Deep Learning Important?\n",
    "\n",
    "Computers have long had techniques for recognizing features inside of images. The results weren’t always great. Computer vision has been a main beneficiary of Deep learning. Computer Vision using Deep Learning now rivals humans on many image recognition tasks.\n",
    "\n",
    "\n",
    "Facebook has had great success with identifying faces in photographs by using deep learning. It’s not just a marginal improvement, but a game changer: “Asked whether two unfamiliar photos of faces show the same person, a human being will get it right 97.53 percent of the time. New software developed by researchers at Facebook can score 97.25 percent on the same challenge, regardless of variations in lighting or whether the person in the picture is directly facing the camera.”\n",
    "\n",
    "\n",
    "Speech recognition is a another area that’s felt deep learning’s impact. Spoken languages are so vast and ambiguous. Baidu – one of the leading search engines of China – has developed a voice recognition system that is faster and more accurate than humans at producing text on a mobile phone. In both English and Mandarin.\n",
    "\n",
    "\n",
    "What is particularly fascinating, is that generalizing the two languages didn’t require much additional design effort: “Historically, people viewed Chinese and English as two vastly different languages, and so there was a need to design very different features,” Andrew Ng says, chief scientist at Baidu. “The learning algorithms are now so general that you can just learn.”\n",
    "\n",
    "Google is now using deep learning to manage the energy at the company’s data centers. They’ve cut their energy needs for cooling by 40%. That translates to about a 15% improvement in power usage efficiency for the company and hundreds of millions of dollars in savings.\n",
    "\n",
    "<img src='images/andrew-ng-chief-scientist-at-baidu-30-638.jpg'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Biological Neurons\n",
    "\n",
    "Biological Neuron, BNN is an unusual-looking cell mostly found in animal cerebral cortexes (e.g., your brain), composed of a cell body containing the nucleus and most of the cell’s complex components, and many branching extensions called Dendrites plus one very long extension called the Axon.\n",
    "\n",
    "The Axon’s length may be just a few times longer than the cell body, or up to tens of thousands of times longer. Near its extremity the axon splits off into many branches called Telodendria, and at the tip of these branches are minuscule structures called Synaptic Terminals (or simply Synapses), which are connected to the dendrites (or directly to the cell body) of other neurons. \n",
    "\n",
    "Biological neurons receive short electrical impulses called Signals from other neurons via these Synapses.\n",
    "When a neuron receives a sufficient number of signals from other neurons within a few milliseconds, it fires its own signals.\n",
    "\n",
    "Thus, individual Biological Neurons seem to behave in a rather simple way, but they are organized in a vast network of billions of neurons, each neuron typically connected to thousands of other neurons.\n",
    "\n",
    "Highly complex computations can be performed by a vast network of fairly simple neurons, much like a complex ant hill can emerge from the combined efforts of simple ants. \n",
    "\n",
    "The architecture of Bilogical Neural Network (BNN) is still the subject of active research, but some parts of the brain have been mapped, and it seems that neurons are often organized in consecutive layers.\n",
    "\n",
    "<img src='images/bnn.jpg'>\n",
    "<img src='images/human bnn.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Logical Computations with Neurons \n",
    "\n",
    "Warren McCulloch and Walter Pitts proposed a very simple model of the Biological Neuron, which later became\n",
    " known as an Artificial Neuron. It has one or more binary (on/off) inputs and one binary output. \n",
    " \n",
    " The artificial neuron simply activates its out‐put when more than a certain number of its inputs are active. \n",
    " \n",
    " McCulloch and Pitts showed that even with such a simplified model it is possible to build a network of artificial neurons that computes any logical proposition you want. \n",
    " \n",
    " For example, let’sbuild a few ANNs that perform various logical computations assuming that a neuron is activated when at least two of its inputs are active.\n",
    "\n",
    "<img src='images/mcculoch-pitts.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perceptrons :  Threshhold Logic Unit (TLU)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " =>  The Perceptrons are one of the simplest ANN architectures, invented in 1957 by Frank Rosen blatt. \n",
    "\n",
    " =>  It is based on a slightly different artificial neuron called a Threshold Logic Unit (TLU), or sometimes \n",
    "       a Linear Threshold Unit (LTU).\n",
    " \n",
    " =>  The input and output are now numbers (instead of binary on/off values) and each input con‐nection is\n",
    "       associated with a weight. \n",
    "        \n",
    " =>  The TLU computes a weighted sum of its inputs (z = w1x1 + w2x2 + ⋯ + wnxn = xTw), then applies a\n",
    "        Step function to that sum and outputs the result.\n",
    "    \n",
    "       ->  hw(x) = step(z), where z = xTw.         # in hw(x), w is index & in xTw, T is Transpose. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/tlu.jpg'>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "  =>  A single TLU can be used for simple linear binary classification. It computes a linear combination of the\n",
    "       inputs and if the result exceeds a threshold, it outputs the positive class or else outputs the \n",
    "         negative class (just like a Logistic Regression classifier or a linear SVM). \n",
    "  \n",
    "      For example, you could use a single TLU to classify iris flowers based on the petal length and width\n",
    "        also adding an extra bias feature (x0 = 1). \n",
    " \n",
    " =>  Training a TLU in this case means finding the right values for w0,w1, and w2.\n",
    "    \n",
    " =>  A Perceptron is simply composed of a single layer of TLUs, with each TLU connected to all the inputs.\n",
    "\n",
    " =>  When all the neurons in a layer are connected to every neuron in the previous layer \n",
    "      (i.e., its input neurons), it is common to draw special pass through neurons called input neurons and\n",
    "     they just output whatever input they are fed. \n",
    "        \n",
    " =>  Moreover, an extra bias feature is generally added (x0 = 1), which is typically represented using a\n",
    "      special type of Neuron called a Bias neuron which just outputs 1 all the time. \n",
    "    \n",
    " =>  A Perceptron with two inputs and three outputs is represented in below figure.\n",
    "     This Perceptron can classifyinstances simultaneously into three different binary classes, which makes it \n",
    "        a multi‐output classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/perceptron-diagram.jpg'>\n",
    "<img src='images/wb.jpg'>\n",
    "\n",
    "###    So how is a Perceptron trained?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Training of a Perceptron\n",
    " \n",
    "> The Perceptron training algorithm proposed by Frank Rosen blatt was largely inspired by Hebb’s rule.\n",
    "     \n",
    "> In his book 'The Organization of Behavior' published in 1949, Donald Hebb suggested that -\n",
    "    **when a Biological Neuron often triggers another neuron, the connection between these two neurons grows \n",
    "        stronger**.\n",
    "            \n",
    "> This idea was later summarized by Siegrid Löwel in this catchy phrase -\n",
    "      'Cells that fire together, wire together' and this rule later became known as Hebb’s rule or Hebbian learning.\n",
    "    \n",
    "> Scikit-Learn provides a Perceptron class that implements a single TLU network. \n",
    "      "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#  Implementing Perceptron class from sk-learn and training on iris dataset\n",
    "#    Note that this solution is generally not unique: in general when the data are linearly separable, \n",
    "#        there is an infinity of hyperplanes that can separate them.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)] # petal length, petal width\n",
    "y = (iris.target == 0).astype(np.int) # Iris Setosa?\n",
    "\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "y_pred = per_clf.predict([[2, 0.5]])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "          ###############   ##############   Multi-layered Perceptrons    ################   ############## \n",
    "\n",
    " =>  An MLP is composed of one (passthrough) input layer, one or more layers of TLUs, called Hidden layers, \n",
    "       and one final layer of TLUs called the Output layer.\n",
    "     \n",
    " =>  The layers close to the input layer are usually called the lower layers, and the ones close to the outputs\n",
    "        are usually called the upper layers. \n",
    "        \n",
    " =>  Every layer except the output layer includes a Bias neuron and is fully connected to the next layer. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/mlp.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Diagram of an Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " =>  There are 3 different types of layers lies inside a Neural Networks.\n",
    "    \n",
    "        * Input Layer    *  Hidden Layers    *  Output Layer\n",
    "    \n",
    " =>  In Input Layer, Number of Perceptrons depends upon the total number of Features available in dataset.\n",
    "    \n",
    " =>  Perceptrons of Input Layer receives the inputted data. In above case, Row 1 is going to be inputted in \n",
    "      Input Layer, then 2nd Row, 3rd Row and so on .\n",
    "    \n",
    " =>  The main function of Input layer is just only getting the inputs from dataset.\n",
    "\n",
    " =>  This Inputted data goes to 2nd Layer of Neural networks (Hidden layer) in such a way that Data from each \n",
    "      Perceptron of Input layer goes inputted to each and every Perceptrons of Hidden layer.\n",
    "\n",
    " =>  When we try to pass our data from Input layer to Hidden layer, We multiply some randomly selected weights,\n",
    "        W1 to inputs X1 and then add some randomly selected Bias, B1 to the inputs. \n",
    "      Then this Linear mix of inputs (W1.X1 + B1) get entered into each Perceptrons of Hidden layer.\n",
    "        \n",
    " =>  Weights and Bias are learnable parameters and they are added to get patterens by making relations \n",
    "       in dataset more easily.\n",
    "\n",
    " =>  If we have only 2 Layers i.e. Input Layer and Output Layer, Whatever data we passes to Input Layer will\n",
    "      be outputted based on Output Functions and with no parameters it is very difficult to find the \n",
    "       relationships between the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/nn.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Feed - Forward Connection"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "      ##############   #############   FFN in Training of Neural networks   ################   ##############\n",
    "    \n",
    " =>  Training of an Artificial Neural Networks has been classified into two diferent processes.\n",
    "       \n",
    "         * Feed Forward Connection (FFN)       &          * Backward Propagation\n",
    "        \n",
    " =>  In Feed Forward Connections, Data is getting inputted into the Input Layer o\n",
    "       Neural Network and then \n",
    "       it get multiplied and added with some randomly selected parameters like as 'weights' and 'Bias'.\n",
    "     \n",
    " =>  Weights & Bias are some Learnable parameters and they are getting introduced into the Networks for\n",
    "       getting an ease of Acces in finding  Relations between data.\n",
    "    \n",
    " =>  After introducing Weights and Bias, a Linear mix of Inputs are getting inputted to the Perceptron of\n",
    "       next layer of Network and there will be similar inputs from all other perceptrons of Input Layer.\n",
    "     So a Summation of all Linear Mix of Inputs will be inputted to each perceptrons of Hidden Layer.\n",
    "    \n",
    " =>  Inside a Hidden layer, a Linear mix of data enters as Input, so we need some particular function which \n",
    "      will be able to Filter, Restrict, Normalize and Non-Linearize the Inputted dataset, which is passed \n",
    "       from Input layer to Hidden layer. ThereFore Activation Function comes into picture. \n",
    "        We can add as many Hidden layers but in a certain limit.\n",
    "        \n",
    " =>  Activation functions are kind of functions, which always helps Perceptrons to fire itself in certain\n",
    "      direction or in a certain range and also try to restricts some sort of dataset.\n",
    "    \n",
    "     Examples of Activation function - \n",
    "\n",
    "        *  Sigmoid Activation Function         *  Tanh Activation Function  \n",
    "        *  RelU Activation Function            *  Leaky RelU Activation function etc.\n",
    "        \n",
    " =>  We can use a Single Activation Function inside a Hidden Layer.\n",
    "      Activation function does some particular operations and give some outputs (say o1, o2), which get \n",
    "        multiplied with some different weights again and then enters into Next Layer.\n",
    "        \n",
    "        \n",
    " =>  In Output Layer, We again uses a Function which is known as Output Function. \n",
    "    Here all Inputs are get added together by using Output function and then we get a final output or\n",
    "      prediction. \n",
    " \n",
    " =>  Output Functions are similar to Activation functions but it always getting used in the output Layer of\n",
    "       the Network.\n",
    "    \n",
    " =>  There will be some differences between the Expected and Predicted outputs, and this difference is\n",
    "      termed as Loss or cost. \n",
    "        \n",
    " =>  This whole process is known as Feed Forward Connection or Feed Forward Networks.\n",
    "      In FFN, our Network does not try to learn something. Learning Never happens in FFN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Assigning Weights and Bias To Inputs   \n",
    "\n",
    "<img src='images/weight.png' width='720px'>\n",
    "   \n",
    "    \n",
    "##   Input of Perceptrons in Hidden layer   \n",
    "\n",
    "<img src='images/total-input.jpg' width='480px'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " =>  In FFN, we get some Losses due to introducing some randomly selected parameters and our whole objective\n",
    "      and idea is to reduce this Loss. For reducing this loss, Backward Propagation comes into picture.\n",
    "    \n",
    " =>  Backward Propagation is a method for efficiently computing the gradient of the cost function of a \n",
    "      Neural network with respect to its parameters. These partial derivatives can then be used to update the \n",
    "        parameters using 'Gradient Descent'.\n",
    "\n",
    " =>  The gradient shows how much the parameters 'weights' & 'Biases' needs to change (in positive or negative \n",
    "        direction) to minimize the Cost function 'C' .\n",
    "    \n",
    " =>  In Backward Propagation, it will try to update the randomly selected parameters by using some \n",
    "      'optimization' mechanism and it always occurs in backward direction and updates the parameters \n",
    "     layer-by-layer in Network on the basis of losses.\n",
    "      For updating the parameters, it uses a Differential equation.\n",
    "        \n",
    " =>  With updated parameters, it will start the Feed Forward Connection again and calculates some Losses\n",
    "      and then it again starts the Backward Propagation to reduce the weights. \n",
    "    It will keep updating the weights untill we get an optimal or reduced Loss and we get some pattern in\n",
    "      dataset.\n",
    "        \n",
    " =>  One Feed Forward connection and one Backward Propagation togetherly makes a cycle, known as one Epoch.\n",
    "\n",
    " =>  In Differential Eqns to update the parameters, we uses a Learning Rate to control the learning process. \n",
    "      Learning Rate should not be too high or too low, it will be in range of (0.001 to 10).\n",
    "        \n",
    " =>  At beginning, We dont have any idea about any relation or patterns between the dataset, so we introduces \n",
    "     some Learnable Parameters and in FFN, a Linear mix of Inputs goes to each and every units of \n",
    "      the Hidden Layer. In Backward Propagation, We updates the  parameters on the basis of Loss.\n",
    "        \n",
    " =>  So with updated parameters, We repeat the FFN and this time Our network drops some data, which are not\n",
    "       contributing in the result and this way, it learns the pattern between the dataset.\n",
    "    \n",
    " =>  Increment of Learnable Parameters makes our Network more flexible to learn the relationship b/w dataset.\n",
    "     Introducing more units in Hidden Layers & more Hidden Layers will give a better result but they are\n",
    "      just experimental ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Chain Rule for updating the parameters\n",
    "\n",
    "The chain rule is a way to compute the derivative of a function whose variables are themselves functions of other variables.  If $C$ is a scalar-valued function of a scalar $z$ and $z$ is itself a scalar-valued function of another scalar variable $w$, then the chain rule states that\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w} = \\frac{\\partial C}{\\partial z}\\frac{\\partial z}{\\partial w}\n",
    "$$\n",
    "For scalar-valued functions of more than one variable, the chain rule essentially becomes additive.  In other words, if $C$ is a scalar-valued function of $N$ variables $z_1, \\ldots, z_N$, each of which is a function of some variable $w$, the chain rule states that\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial w} = \\sum_{i = 1}^N \\frac{\\partial C}{\\partial z_i}\\frac{\\partial z_i}{\\partial w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/eqn.jpg' width = '420px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "In the following derivation, we'll use the following notation:\n",
    "\n",
    "$L$ - Number of layers in the network.\n",
    "\n",
    "$N^n$ - Dimensionality of layer $n \\in \\{0, \\ldots, L\\}$.  $N^0$ is the dimensionality of the input; $N^L$ is the dimensionality of the output.\n",
    "\n",
    "$W^m \\in \\mathbb{R}^{N^m \\times N^{m - 1}}$ - Weight matrix for layer $m \\in \\{1, \\ldots, L\\}$.  $W^m_{ij}$ is the weight between the $i^{th}$ unit in layer $m$ and the $j^{th}$ unit in layer $m - 1$.\n",
    "\n",
    "$b^m \\in \\mathbb{R}^{N^m}$ - Bias vector for layer $m$.\n",
    "\n",
    "$\\sigma^m$ - Nonlinear activation function of the units in layer $m$, applied elementwise.\n",
    "\n",
    "$z^m \\in \\mathbb{R}^{N^m}$ - Linear mix of the inputs to layer $m$, computed by $z^m = W^m a^{m - 1} + b^m$.\n",
    "\n",
    "$a^m \\in \\mathbb{R}^{N^m}$ - Activation of units in layer $m$, computed by $a^m = \\sigma^m(h^m) = \\sigma^m(W^m a^{m - 1} + b^m)$.  $a^L$ is the output of the network.  We define the special case $a^0$ as the input of the network.\n",
    "\n",
    "$y \\in \\mathbb{R}^{N^L}$ - Target output of the network.\n",
    "\n",
    "$C$ - Cost/error function of the network, which is a function of $a^L$ (the network output) and $y$ (treated as a constant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/Ann.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Neural Networks Training Blog](https://towardsdatascience.com/understanding-backpropagation-algorithm-7bb3aa2f95fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation in general\n",
    "\n",
    "In order to train the network using a gradient descent algorithm, we need to know the gradient of each of the parameters with respect to the cost/error function $C$; that is, we need to know $\\frac{\\partial C}{\\partial W^m}$ and $\\frac{\\partial C}{\\partial b^m}$.  It will be sufficient to derive an expression for these gradients in terms of the following terms, which we can compute based on the neural network's architecture:\n",
    "\n",
    "- $\\frac{\\partial C}{\\partial a^L}$: The derivative of the cost function with respect to its argument, the output of the network\n",
    "- $\\frac{\\partial a^m}{\\partial z^m}$: The derivative of the nonlinearity used in layer $m$ with respect to its argument\n",
    "\n",
    "To compute the gradient of our cost/error function $C$ to $W^m_{ij}$ (a single entry in the weight matrix of the layer $m$), we can first note that $C$ is a function of $a^L$, which is itself a function of the linear mix variables $z^m_k$, which are themselves functions of the weight matrices $W^m$ and biases $b^m$.  With this in mind, we can use the chain rule as follows:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial W^m_{ij}} = \\sum_{k = 1}^{N^m} \\frac{\\partial C}{\\partial z^m_k} \\frac{\\partial z^m_k}{\\partial W^m_{ij}}$$\n",
    "\n",
    "Note that by definition \n",
    "$$\n",
    "z^m_k = \\sum_{l = 1}^{N^m} W^m_{kl} a_l^{m - 1} + b^m_k\n",
    "$$\n",
    "It follows that $\\frac{\\partial z^m_k}{\\partial W^m_{ij}}$ will evaluate to zero when $i \\ne k$ because $z^m_k$ does not interact with any elements in $W^m$ except for those in the $k$<sup>th</sup> row, and we are only considering the entry $W^m_{ij}$.  When $i = k$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial z^m_i}{\\partial W^m_{ij}} &= \\frac{\\partial}{\\partial W^m_{ij}}\\left(\\sum_{l = 1}^{N^m} W^m_{il} a_l^{m - 1} + b^m_i\\right)\\\\\n",
    "&= a^{m - 1}_j\\\\\n",
    "\\rightarrow \\frac{\\partial z^m_k}{\\partial W^m_{ij}} &= \\begin{cases}\n",
    "0 & k \\ne i\\\\\n",
    "a^{m - 1}_j & k = i\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "The fact that $\\frac{\\partial C}{\\partial a^m_k}$ is $0$ unless $k = i$ causes the summation above to collapse, giving\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial W^m_{ij}} = \\frac{\\partial C}{\\partial z^m_i} a^{m - 1}_j$$\n",
    "\n",
    "or in vector form\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial W^m} = \\frac{\\partial C}{\\partial z^m} a^{m - 1 \\top}$$\n",
    "\n",
    "Similarly for the bias variables $b^m$, we have\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^m_i} = \\sum_{k = 1}^{N^m} \\frac{\\partial C}{\\partial z^m_k} \\frac{\\partial z^m_k}{\\partial b^m_i}$$\n",
    "\n",
    "As above, it follows that $\\frac{\\partial z^m_k}{\\partial b^m_i}$ will evaluate to zero when $i \\ne k$ because $z^m_k$ does not interact with any element in $b^m$ except $b^m_k$.  When $i = k$, we have\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial z^m_i}{\\partial b^m_i} &= \\frac{\\partial}{\\partial b^m_i}\\left(\\sum_{l = 1}^{N^m} W^m_{il} a_l^{m - 1} + b^m_i\\right)\\\\\n",
    "&= 1\\\\\n",
    "\\rightarrow \\frac{\\partial z^m_i}{\\partial b^m_i} &= \\begin{cases}\n",
    "0 & k \\ne i\\\\\n",
    "1 & k = i\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "The summation also collapses to give\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^m_i} = \\frac{\\partial C}{\\partial z^m_i}$$\n",
    "\n",
    "or in vector form\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial b^m} = \\frac{\\partial C}{\\partial z^m}$$\n",
    "\n",
    "Now, we must compute $\\frac{\\partial C}{\\partial z^m_k}$.  For the final layer ($m = L$), this term is straightforward to compute using the chain rule:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial z^L_k} = \\frac{\\partial C}{\\partial a^L_k} \\frac{\\partial a^L_k}{\\partial z^L_k}\n",
    "$$\n",
    "\n",
    "or, in vector form\n",
    "\n",
    "$$\n",
    "\\frac{\\partial C}{\\partial z^L} = \\frac{\\partial C}{\\partial a^L} \\frac{\\partial a^L}{\\partial z^L}\n",
    "$$\n",
    "\n",
    "The first term $\\frac{\\partial C}{\\partial a^L}$ is just the derivative of the cost function with respect to its argument, whose form depends on the cost function chosen.  Similarly, $\\frac{\\partial a^m}{\\partial z^m}$ (for any layer $m$ includling $L$) is the derivative of the layer's nonlinearity with respect to its argument and will depend on the choice of nonlinearity.  For other layers, we again invoke the chain rule:\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial C}{\\partial z^m_k} &= \\frac{\\partial C}{\\partial a^m_k} \\frac{\\partial a^m_k}{\\partial z^m_k}\\\\\n",
    "&= \\left(\\sum_{l = 1}^{N^{m + 1}}\\frac{\\partial C}{\\partial z^{m + 1}_l}\\frac{\\partial z^{m + 1}_l}{\\partial a^m_k}\\right)\\frac{\\partial a^m_k}{\\partial z^m_k}\\\\\n",
    "&= \\left(\\sum_{l = 1}^{N^{m + 1}}\\frac{\\partial C}{\\partial z^{m + 1}_l}\\frac{\\partial}{\\partial a^m_k} \\left(\\sum_{h = 1}^{N^m} W^{m + 1}_{lh} a_h^m + b_l^{m + 1}\\right)\\right) \\frac{\\partial a^m_k}{\\partial z^m_k}\\\\\n",
    "&= \\left(\\sum_{l = 1}^{N^{m + 1}}\\frac{\\partial C}{\\partial z^{m + 1}_l} W^{m + 1}_{lk}\\right) \\frac{\\partial a^m_k}{\\partial z^m_k}\\\\\n",
    "&= \\left(\\sum_{l = 1}^{N^{m + 1}}W^{m + 1\\top}_{kl} \\frac{\\partial C}{\\partial z^{m + 1}_l}\\right) \\frac{\\partial a^m_k}{\\partial z^m_k}\\\\\n",
    "\\end{align*}\n",
    "\n",
    "where the last simplification was made because by convention $\\frac{\\partial C}{\\partial z^{m + 1}_l}$ is a column vector, allowing us to write the following vector form:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial z^m} = \\left(W^{m + 1\\top} \\frac{\\partial C}{\\partial z^{m + 1}}\\right) \\circ \\frac{\\partial a^m}{\\partial z^m}$$\n",
    "\n",
    "Note that we now have the ingredients to efficiently compute the gradient of the cost function with respect to the network's parameters:  First, we compute $\\frac{\\partial C}{\\partial z^L_k}$ based on the choice of cost function and nonlinearity.  Then, we recursively can compute $\\frac{\\partial C}{\\partial z^m}$ layer-by-layer based on the term $\\frac{\\partial C}{\\partial z^{m + 1}}$ computed from the previous layer and the nonlinearity of the layer (this is called the \"backward pass\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation in practice\n",
    "\n",
    "As discussed above, the exact form of the updates depends on both the chosen cost function and each layer's chosen nonlinearity.  The following two table lists the some common choices for nonlinearities and the required partial derivative for deriving the gradient for each layer:\n",
    "\n",
    "| Nonlinearity | $a^m = \\sigma^m(z^m)$ | $\\frac{\\partial a^m}{\\partial z^m}$ | Notes |\n",
    "|--------------|---|---|---|\n",
    "| Sigmoid      | $\\frac{1}{1 + e^{z^m}}$ | $\\sigma^m(z^m)(1 - \\sigma^m(z^m)) = a^m(1 - a^m)$ | \"Squashes\" any input to the range $[0, 1]$ |\n",
    "| Tanh         | $\\frac{e^{z^m} - e^{-z^m}}{e^{z^m} + e^{-z^m}}$ | $1 - (\\sigma^m(z^m))^2 = 1 - (a^m)^2$ | Equivalent, up to scaling, to the sigmoid function |\n",
    "| ReLU         | $\\max(0, z^m)$ | $0, z^m < 0;\\; 1, z^m \\ge 0$ | Commonly used in neural networks with many layers|\n",
    "\n",
    "Similarly, the following table collects some common cost functions and the partial derivative needed to compute the gradient for the final layer:\n",
    "\n",
    "| Cost Function | $C$                                  | $\\frac{\\partial C}{\\partial a^L}$ | Notes |\n",
    "|---------------|--------------------------------------|-----------------------------------|---|\n",
    "| Squared Error | $\\frac{1}{2}(y - a^L)^\\top(y - a^L)$ | $y - a^L$                         | Commonly used when the output is not constrained to a specific range |\n",
    "| Cross-Entropy | $(y - 1)\\log(1 - a^L) - y\\log(a^L)$  | $\\frac{a^L - y}{a^L(1 - a^L)}$    | Commonly used for binary classification tasks; can yield faster convergence |\n",
    "\n",
    "In practice, backpropagation proceeds in the following manner for each training sample:\n",
    "\n",
    "1. Forward pass: Given the network input $a^0$, compute $a^m$ recursively by\n",
    " $$a^1 = \\sigma^1(W^1 a^0 + b^1), \\ldots, a^L = \\sigma^L(W^L a^{L - 1} + b^L)$$\n",
    "1. Backward pass: Compute \n",
    "$$\\frac{\\partial C}{\\partial z^L} = \\frac{\\partial C}{\\partial a^L} \\frac{\\partial a^L}{\\partial z^L}$$\n",
    "for the final layer based on the tables above, then recursively compute\n",
    "$$\\frac{\\partial C}{\\partial z^m} = \\left(W^{m + 1\\top} \\frac{\\partial C}{\\partial z^{m + 1}}\\right) \\circ \\frac{\\partial a^m}{\\partial z^m}$$\n",
    "for all other layers.  Plug these values into \n",
    "$$\\frac{\\partial C}{\\partial W^m} = \\frac{\\partial C}{\\partial z^m_i} a^{m - 1 \\top}$$\n",
    "and\n",
    "$$\\frac{\\partial C}{\\partial b^m} = \\frac{\\partial C}{\\partial z^m}$$\n",
    "to obtain the updates.\n",
    "\n",
    "### Example: Sigmoid network with cross-entropy loss using gradient descent\n",
    "\n",
    "A common network architecture is one with fully connected layers where each layer's nonlinearity is the sigmoid function $a^m = \\frac{1}{1 + e^{z^m}}$ and the cost function is the cross-entropy loss $(y - 1)\\log(1 - a^L) - y\\log(a^L)$.  To compute the updates for gradient descent, we first compute (based on the tables above)\n",
    "\\begin{align*}\n",
    "\\frac{\\partial C}{\\partial z^L} &= \\frac{\\partial C}{\\partial a^L} \\frac{\\partial a^L}{\\partial z^L}\\\\\n",
    "&= \\left(\\frac{a^L - y}{a^L(1 - a^L)}\\right)a^L(1 - a^L)\\\\\n",
    "&= a^L - y\n",
    "\\end{align*}\n",
    "From here, we can compute\n",
    "\\begin{align*}\n",
    "\\frac{\\partial C}{\\partial z^{L - 1}} &= \\left(W^{L\\top} \\frac{\\partial C}{\\partial z^L} \\right) \\circ \\frac{\\partial a^{L - 1}}{\\partial z^{L - 1}}\\\\\n",
    "&= W^{L\\top} (a^L - y) \\circ a^{L - 1}(1 - a^{L - 1})\\\\\n",
    "\\frac{\\partial C}{\\partial z^{L - 2}} &= \\left(W^{L - 1\\top} \\frac{\\partial C}{\\partial z^{L - 1}} \\right) \\circ \\frac{\\partial a^{L - 2}}{\\partial z^{L - 2}}\\\\\n",
    "&= W^{L - 1\\top} \\left(W^{L\\top} (a^L - y) \\circ a^{L - 1}(1 - a^{L - 1})\\right) \\circ a^{L - 2}(1 - a^{L - 2})\n",
    "\\end{align*}\n",
    "and so on, until we have computed $\\frac{\\partial C}{\\partial z^m}$ for $m \\in \\{1, \\ldots, L\\}$.  This allows us to compute  $\\frac{\\partial C}{\\partial W^m_{ij}}$ and $\\frac{\\partial C}{\\partial b^m_i}$, e.g.\n",
    "\\begin{align*}\n",
    "\\frac{\\partial C}{\\partial W^L} &= \\frac{\\partial C}{\\partial z^L} a^{L - 1 \\top}\\\\\n",
    "&= (a^L - y)a^{L - 1\\top}\\\\\n",
    "\\frac{\\partial C}{\\partial W^{L - 1}} &= \\frac{\\partial C}{\\partial z^{L - 1}} a^{L - 2 \\top}\\\\\n",
    "&= W^{L\\top} (a^L - y) \\circ a^{L - 1}(1 - a^{L - 1}) a^{L - 2\\top}\n",
    "\\end{align*}\n",
    "and so on.  Standard gradient descent then updates each parameter as follows:\n",
    "$$W^m = W^m - \\lambda \\frac{\\partial C}{\\partial W^m}$$\n",
    "$$b^m = b^m - \\lambda \\frac{\\partial C}{\\partial b^m}$$\n",
    "where $\\lambda$ is the learning rate.  This process is repeated until some stopping criteria is met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent\n",
    "---\n",
    "The simplest algorithm for iterative minimization of differentiable functions is known as just **gradient descent**.\n",
    "Recall that the gradient of a function is defined as the vector of partial derivatives:\n",
    "\n",
    "$$\\nabla f(x) =  [{\\partial{f}{x_1}, \\partial{f}{x_2}, \\ldots, \\partial{f}{x_n}}]$$\n",
    "\n",
    "and that the gradient of a function always points towards the direction of maximal increase at that point.\n",
    "\n",
    "Equivalently, it points *away* from the direction of maximum decrease - thus, if we start at any point, and keep moving in the direction of the negative gradient, we will eventually reach a local minimum.\n",
    "\n",
    "This simple insight leads to the Gradient Descent algorithm. Outlined algorithmically, it looks like this:\n",
    "\n",
    "1. Pick a point $x_0$ as your initial guess.\n",
    "2. Compute the gradient at your current guess:\n",
    "$v_i = \\nabla f(x_i)$\n",
    "3. Move by $\\alpha$ (your step size) in the direction of that gradient:\n",
    "$x_{i+1} = x_i + \\alpha v_i$\n",
    "4. Repeat steps 1-3 until your function is close enough to zero (until $f(x_i) < \\varepsilon$ for some small tolerance $\\varepsilon$)\n",
    "\n",
    "Note that the step size, $\\alpha$, is simply a parameter of the algorithm and has to be fixed in advance. \n",
    "\n",
    "![gd](http://ludovicarnold.altervista.org/wp-content/uploads/2015/01/gradient-trajectory.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the hyperbolic tangent function asymptotes at -1 and 1, rather than 0 and 1, which is sometimes beneficial, and its derivative is simple:\n",
    "\n",
    "$$\\frac{d \\tanh(x)}{dx} = 1 - \\tanh^2(x)$$\n",
    "\n",
    "Performing gradient descent will allow us to change the weights in the direction that optimially reduces the error. The next trick will be to employ the **chain rule** to decompose how the error changes as a function of the input weights into the change in error as a function of changes in the inputs to the weights, mutliplied by the changes in input values as a function of changes in the weights. \n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w} = \\frac{\\partial E}{\\partial h}\\frac{\\partial h}{\\partial w}$$\n",
    "\n",
    "This will allow us to write a function describing the activations of the output weights as a function of the activations of the hidden layer nodes and the output weights, which will allow us to propagate error backwards through the network.\n",
    "\n",
    "The second term in the chain rule simplifies to:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial h_k}{\\partial w_{jk}} &= \\frac{\\partial \\sum_l w_{lk} a_l}{\\partial w_{jk}}  \\\\\n",
    "&= \\sum_l \\frac{\\partial w_{lk} a_l}{\\partial w_{jk}} \\\\\n",
    "& = a_j\n",
    "\\end{align}$$\n",
    "\n",
    "where $a_j$ is the activation of the jth hidden layer neuron.\n",
    "\n",
    "For the first term in the chain rule above, we decompose it as well:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial h_k} = \\frac{\\partial E}{\\partial y_k}\\frac{\\partial y_k}{\\partial h_k} = \\frac{\\partial E}{\\partial g(h_k)}\\frac{\\partial g(h_k)}{\\partial h_k}$$\n",
    "\n",
    "The second term of this chain rule is just the derivative of the activation function, which we have chosen to have a conveneint form, while the first term simplifies to:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial g(h_k)} = \\frac{\\partial}{\\partial g(h_k)}\\left[\\frac{1}{2} \\sum_k (t_k - y_k)^2 \\right] = t_k - y_k$$\n",
    "\n",
    "Combining these, and assuming (for illustration) a logistic activiation function, we have the gradient:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w} = (t_k - y_k) y_k (1-y_k) a_j$$\n",
    "\n",
    "Which ends up getting plugged into the weight update formula that we saw in the single-layer perceptron:\n",
    "\n",
    "$$w_{jk} \\leftarrow w_{jk} - \\eta (t_k - y_k) y_k (1-y_k) a_j$$\n",
    "\n",
    "Note that here we are *subtracting* the second term, rather than adding, since we are doing gradient descent.\n",
    "\n",
    "We can now outline the MLP learning algorithm:\n",
    "\n",
    "1. Initialize all $w_{jk}$ to small random values\n",
    "2. For each input vector, conduct forward propagation:\n",
    "    * compute activation of each neuron $j$ in hidden layer (here, sigmoid):\n",
    "    $$h_j = \\sum_i x_i v_{ij}$$\n",
    "    $$a_j = g(h_j) = \\frac{1}{1 + \\exp(-\\beta h_j)}$$\n",
    "    * when the output layer is reached, calculate outputs similarly:\n",
    "    $$h_k = \\sum_k a_j w_{jk}$$\n",
    "    $$y_k = g(h_k) = \\frac{1}{1 + \\exp(-\\beta h_k)}$$\n",
    "3. Calculate loss for resulting predictions:\n",
    "    * compute error at output:\n",
    "    $$\\delta_k = (t_k - y_k) y_k (1-y_k)$$\n",
    "4. Conduct backpropagation to get partial derivatives of cost with respect to weights, and use these to update weights:\n",
    "    * compute error of the hidden layers:\n",
    "    $$\\delta_{hj} = \\left[\\sum_k w_{jk} \\delta_k \\right] a_j(1-a_j)$$\n",
    "    * update output layer weights:\n",
    "    $$w_{jk} \\leftarrow w_{jk} - \\eta \\delta_k a_j$$\n",
    "    * update hidden layer weights:\n",
    "    $$v_{ij} \\leftarrow v_{ij} - \\eta \\delta_{hj} x_i$$\n",
    "    \n",
    "Return to (2) and iterate until learning completes. Best practice is to shuffle input vectors to avoid training in the same order.\n",
    "\n",
    "Its important to be aware that because gradient descent is a hill-climbing (or descending) algorithm, it is liable to be caught in local minima with respect to starting values. Therefore, it is worthwhile training several networks using a range of starting values for the weights, so that you have a better chance of discovering a globally-competitive solution.\n",
    "\n",
    "One useful performance enhancement for the MLP learning algorithm is the addition of **momentum** to the weight updates. This is just a coefficient on the previous weight update that increases the correlation between the current weight and the weight after the next update. This is particularly useful for complex models, where falling into local mimima is an issue; adding momentum will give some weight to the previous direction, making the resulting weights essentially a weighted average of the two directions. Adding momentum, along with a smaller learning rate, usually results in a more stable algorithm with quicker convergence. When we use momentum, we lose this guarantee, but this is generally seen as a small price to pay for the improvement momentum usually gives.\n",
    "\n",
    "A weight update with momentum looks like this:\n",
    "\n",
    "$$w_{jk} \\leftarrow w_{jk} - \\eta \\delta_k a_j + \\alpha \\Delta w_{jk}^{t-1}$$\n",
    "\n",
    "where $\\alpha$ is the momentum (regularization) parameter and $\\Delta w_{jk}^{t-1}$ the update from the previous iteration.\n",
    "\n",
    "The multi-layer pereptron is implemented below in the `MLP` class. The implementation uses the scikit-learn interface, so it is uses in the same way as other supervised learning algorithms in that package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Activation Functions are a kind of function, which helps a Neuron to fire itself in a certain direction, in a  certain bandwidth, or in a certain range. It also try to restrict some sort of dataset.\n",
    "    \n",
    "> Activation functions helps to determine the output of a Neural Network. These type of functions are attached to each Neurons in the Network, and determines whether it should be activated or not, based on whether each neuron’s input is relevant for the model’s prediction. \n",
    "\n",
    "> Activation Function also helps to normalize the output of each neuron to a range between (1 and 0) or between \n",
    "    (1 and -1).\n",
    "\n",
    "> Neural Networks use non-linear Activation Functions, which can help the network learn complex data, compute and learn almost any function representing a question, and provide accurate predictions.\n",
    "\n",
    "> The activation function is a mathematical “gate” in between the input feeding the current neuron and its output going to the next layer. It can be as simple as a step function that turns the neuron output on and off, depending on a rule or threshold.\n",
    "\n",
    "<img src='images/1.jpg' width='400px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  There are 3 different types of Activation Functions available to be used in Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Activation functions**\n",
    "\n",
    "> **Linear Activation functions** always defines a Linear relationship  'Y = m.X + c'. Differentials of a Linear function gives some constant, so it can be used in Back-propagation. But a linear activation function is not able to handle the complexity of data like as it will get failed in cases of Image recognitions or more. \n",
    "     \n",
    "<img src='images/linear.png' width='240px'>\n",
    "     \n",
    "**Binary Step Activation functions**\n",
    "\n",
    "> **Binary Step Activation functions** always gives it's output in a range (0 and 1). For Negative X, it always gives 0 and for Positive X, i gives 1. It can handle 2 classes.\n",
    " But it's derivative is always 0, so this function is not proper for using in Back-propagation.\n",
    "     \n",
    "<img src='images/binary-step.png' width='380px'>\n",
    "     \n",
    " **Non-linear Activation functions**\n",
    "     \n",
    ">  Non Linear Activation functions are able to understand the complexity of dataset and also it gives some differentials, which gets used in Back-propagation to update the parameters. So We generally uses Non-linear Activation functions in Deep learning.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "####  Some popular Activation functions used in Deep Learning\n",
    "\n",
    "     * Sigmoid Function                        * Tanh Function\n",
    "     * ReLU Function                           * Leaky RelU Function\n",
    "     * PReLU Function                          * ELU Function\n",
    "     * Softmax Function                        * SoftPlus Function\n",
    "     * Swish Function                          * Maxout Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Sigmod  or Logistic Activation Function\n",
    "\n",
    "> The Sigmoid function is the most frequently used Activation function in the beginning of Deep learning.\n",
    "> For every Input value, it gives Output in a range ∈ (0,1). \n",
    "<img src='images/sig.svg' width='200px'>\n",
    "\n",
    "\n",
    "####  Calculation of Derivative of Sigmoid function\n",
    "\n",
    "<img src='images/sigmoid-derivative.jpg' width='480px'>\n",
    "<img src='images/sigmoid-derivative.png' width='320px'>\n",
    "\n",
    "\n",
    "### Graph of Sigmoid Function and it's Derivative\n",
    "\n",
    "<img src='images/sig.jpg' width='480px'>\n",
    "\n",
    "> Advantages of Sigmoid Function : -\n",
    "\n",
    "1. Smooth gradient, preventing “jumps” in output values.\n",
    "2. Output values bound between 0 and 1, normalizing the output of each neuron.\n",
    "3. Clear predictions, i.e very close to 0 or 1.\n",
    "\n",
    "\n",
    "> Sigmoid has 4 major disadvantages:\n",
    "* Leads to Vanishing Gradient problem\n",
    "* Leads to Gradient Saturation or dispersing\n",
    "* Function output is not zero-centered\n",
    "* Time consuming  and Slower for computers due to using Exponential operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Vanishing Gradient Problem\n",
    "\n",
    "> In Back Propagation, Derivatives of each previous layers have some contribution in updating the weights of next layers and We uses Chain rule to update the weights. And Derivatives of Sigmoid Functions ranges between (0 to 0.25).\n",
    "\n",
    "> When Sigmoid functions are getting used as Activation functions in Deep neural networks, it's Derivative are getting decreases during updating of weights in each layers. In case of a Deep neural networks, When Back-propagation reaches to the last layer, Derivative of Sigmoid Function gives a very small value (near to 0) and so there will be very minute change or no changes in the weights in the last layer of Network. This problem is known as Vanishing Gradient problem.  \n",
    "\n",
    "> In 1980s or 1990s, Sigmoid function is the only Activation function, that people uses and they don't have too many layers in their Neural networks. They just deal with 2 or 3 layered Neural networks to avoiding this Vanishing Gradient problem.\n",
    "\n",
    "> Visit [Blog](https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484)\n",
    "        [wikipedia](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)\n",
    "        \n",
    "\n",
    "###  Non Zero - Centric Problem\n",
    " \n",
    "> Functions having it's centre lying on the origin are Termed as Zero - Centric Functions.\n",
    "    Sigmoid function is not  Zero - Centric Function.\n",
    "\n",
    ">  When we uses a Zero - Centric function as Activation Function, it  will try to control the changes or updates in gradients in backward direction. \n",
    "\n",
    ">  In case of a Non Zero-centric Function, Gradient updates will goes in different - different directions and it will make our optimisation much more harder because it is not able to maintain the consistency in Gradient updates and so every time it's value fluctuates and gives a different value.\n",
    "\n",
    "###   Gradient Saturation or Gradient Dispersing Problem\n",
    "\n",
    "> When inputs are away from the coordinate origin, the gradient of the function becomes very small, almost zero. \n",
    "In the process of neural network backpropagation, we all use the chain rule of differential to calculate the differential of each weight w. When the backpropagation passes through the sigmod function, the differential on this chain is very small. Moreover, it may pass through many sigmod functions, which will eventually cause the weight w to have little effect on the loss function, which is not conducive to the optimization of the weight. This The problem is called Gradient Saturation or Gradient Dispersion.\n",
    "\n",
    " [visit here](https://programmersought.com/article/21921360722/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Tanh or Hyperbolic Tangent Function\n",
    "\n",
    "> Tanh or Hyperbolic Tangent function is modified version of the Sigmoid Function. \n",
    "The output interval of tanh is [-1 & 1] and the whole function is 0-centric, which is better than Sigmoid.\n",
    "<img src='images/tan.svg' width='200px'>\n",
    "<img src='images/sig-tan.jpg' width='480px'>\n",
    "\n",
    "> Tanh function can be represented in terms of Sigmoid function.\n",
    "<img src='images/tan-sigmoid.jpg' width='420px'>\n",
    "\n",
    "####  Derivative of TanH Function\n",
    "\n",
    "> For calculating the derivative of Tanh Function, we uses u/v rule. \n",
    "<img src='images/tan.jpg' width='240px'>\n",
    "<img src='images/tan-derivative.jpg' width='420px'>\n",
    "\n",
    "> In general Binary classification problems, Tanh function is used for the hidden layer and the Sigmoid function is used for the output layer. However, these are not static, and the specific activation function to be used must be analyzed according to the specific problem, or it depends on debugging.\n",
    "\n",
    "> Comparing to Sigmoid Function, Tanh is a Zero centerd function but still it leads to Vanishing Gradient problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Rectified  Linear  Unit  or  ReLU  Function\n",
    "\n",
    "> ReLU or Rectified Linear Unit function is defined as a function that takes the maximum value and outputs in a range (0, +infinity), and this function is not fully interval-derivable, but we can take sub-gradients.  \n",
    "<img src='images/relu-fn.jpg' width='420px'>\n",
    "<img src='images/relu-der.jpg' width='420px'>\n",
    "<img src='images/relu.png' width='520px'>\n",
    "\n",
    "\n",
    "> Advantages of ReLU Function.\n",
    "*  Overcome with Gradient Saturation or Vanishing Gradient problem. When the input is Positive, it always gives a Gradient but in case of Negative input, it gives Y as 0, so no any Gradient.\n",
    "\n",
    "* ReLU Function not activates all Neurons at a time. Means that, If any Linear combination of inputs are negative, then it will restrict that Neuron to fire itself and gives output as 0. May be in Next iteration, if there will be Positive input, then it will Fire the Neuron. This property is very important in Feed Forward Connection. \n",
    "\n",
    "*  ReLU calculation speed is much faster because it has only a Linear relationship (forward or backward) and so it is much faster than Sigmod and Tanh. (Sigmod and tanh need to calculate the exponent, which results in slower calculation speed.)\n",
    "\n",
    "> Disadvantages of ReLU Function\n",
    " *  When the input is Negative, ReLU is completely inactive, which means that once a negative number is entered, ReLU will die. In this way, in the forward propagation process, it is not a problem. Some areas are sensitive and some are insensitive. But in the back propagation process, if you enter a negative number, the gradient will be completely zero, which has the same problem as the Sigmoid function and Tanh function.\n",
    "\n",
    " *  ReLU Function is not a Zero Centric function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Leaky  ReLU  Function\n",
    "\n",
    "> Leaky ReLU Function is a modified version of ReLU Function. For Negative Inputs, It gives some values of Y.\n",
    "In order to solve the Dead ReLU Problem, people proposed to set the first half of ReLU 0.01x instead of 0.\n",
    "\n",
    ">  Leaky ReLU has all the advantages of ReLU, plus there will be no problems with Dead ReLU, but in actual operation, it has not been fully proved that Leaky ReLU is always better than ReLU. \n",
    "\n",
    "\n",
    "<img src='images/lrelu.svg' width='240px'>\n",
    "<img src='images/leaky-relu.png' width='480px'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  PRelu (Parametric ReLU)\n",
    "\n",
    "> Parametric ReLU or PReLU Function is improvised version of ReLU Function. In the negative region, PReLU has a small slope, which can also avoid the problem of ReLU death. Compared to ELU, PReLU is a linear operation in the negative region. Although the slope is small, it does not tend to 0, which is a certain advantage.\n",
    "\n",
    "<img src='images/Capture.PNG' width='480px'>\n",
    "\n",
    "> In PRelu, alpha 'α' is a learnable parameter, which it learns by tuning..\n",
    "\n",
    "* if α = 0, f becomes ReLU\n",
    "* if α > 0, f becomes Leaky ReLU\n",
    "* if α is a learnable parameter, f becomes PReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ELU (Exponential Linear Units) function\n",
    "\n",
    "> ELU is also proposed to solve the problems of ReLU. \n",
    "\n",
    "<img src='images/elu.svg' width='280px'>\n",
    "<img src='images/elu.jpg' width='520px'>\n",
    "\n",
    "\n",
    ">  Advantages of ELU Function\n",
    "\n",
    "* No dead Neurons issues, if input is Negative\n",
    "* ELU is a Zero-centric Function means that Mean of the output is close to 0.\n",
    "\n",
    ">  Disadvantages of ELU Functions\n",
    "\n",
    "*  For negative inputs, ELU computation is expensively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax function\n",
    "\n",
    "> Softmax function is used for Multi class classifications in output layers. It is used for getting the probabilities for different multiple classes among all. It always gives an output in a range [0,1].\n",
    "\n",
    "####  Softmax Function is defined as \n",
    "<img src='images/soft.png'>\n",
    "\n",
    "> Softmax is different from the normal max function: the max function only outputs the largest value, and Softmax ensures that smaller values have a smaller probability and will not be discarded directly. It is a \"max\" that is \"soft\".\n",
    "\n",
    "> The denominator of the Softmax function combines all factors of the original output value, which means that the different probabilities obtained by the Softmax function are related to each other.\n",
    "\n",
    "> In the case of binary classification, for Sigmoid, there are:\n",
    "<img src='images/soft1.png'>\n",
    "\n",
    "\n",
    "> For Softmax with K = 2, there are:\n",
    "<img src='images/soft2.png'>\n",
    "\n",
    "> It can be seen that in the case of binary classification, Softmax is degraded to Sigmoid.\n",
    "<img src='images/soft4.png' width='240px'>\n",
    "   \n",
    "###  Calculation of probabilities using Softmax Function  \n",
    "\n",
    "<img src='images/prob-softmax.jpg' width='720px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/soft31.png' width='580px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Swish (A Self-Gated) Function\n",
    "\n",
    ">  Swish Function is derived from the Sigmoid Function itself and it is defined as =>  **y = x * sigmoid (x)**\n",
    "\n",
    "> Swish's design was inspired by the use of sigmoid functions for gating in LSTMs and Highway networks. We use the same value for gating to simplify the gating mechanism, which is called **self-gating**. \n",
    "\n",
    "> The advantage of self-gating is that it only requires a simple scalar input, while normal gating requires multiple scalar inputs. This feature enables self-gated activation functions such as Swish to easily replace activation functions that take a single scalar as input (such as ReLU) without changing the hidden capacity or number of parameters.\n",
    "\n",
    "<img src='images/swish.png' width='420px'\n",
    "     >\n",
    "\n",
    "####  Advantages of Swish Function\n",
    "\n",
    "*  Unboundedness (unboundedness) is helpful to prevent gradient from gradually approaching '0' during slow training, causing Saturation. At the same time, being bounded has advantages, because bounded active functions can have strong regularization, and larger negative inputs will be resolved.\n",
    "\n",
    "* At the same time, smoothness also plays an important role in optimization and generalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  SoftPlus Activation Function\n",
    "\n",
    "> The softplus function is similar to the ReLU function, but it is relatively smooth.It is unilateral suppression like ReLU. It has a wide acceptance range (0, + inf).\n",
    "\n",
    "<img src='images/softPlus.jpg' width='360px'>\n",
    "<img src='images/softplus.png' width='520px'>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Maxout Activation Function\n",
    "\n",
    "> The Maxout activation function don't goes with the linear combination inputs, but it gives the maximumm input among all different inputs coming from various Neurons.\n",
    "<img src='images/max.jpeg' width='240px'>\n",
    "\n",
    "> One relatively popular choice is the Maxout neuron (introduced recently by Goodfellow et al.) that generalizes the ReLU and its leaky version. Notice that both ReLU and Leaky ReLU are a special case of this form (for example, for ReLU we have w1,b1 =0).The Maxout neuron therefore enjoys all the benefits of a ReLU unit (linear regime of operation, no saturation) and does not have its drawbacks\n",
    "\n",
    "\n",
    "> The Maxout activation is a generalization of the ReLU and the leaky ReLU functions. It is a learnable activation function.\n",
    "\n",
    "> Maxout can be seen as adding a layer of activation function to the deep learning network, which contains a parameter k. Compared with ReLU, sigmoid, etc., this layer is special in that it adds k neurons and then outputs the largest activation value. value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Generally speaking, these activation functions have their own advantages and disadvantages. There is no statement that indicates which ones are not working, and which activation functions are good. All the good and bad must be obtained by experiments.\n",
    "\n",
    "> [Blog](https://medium.com/@omkar.nallagoni/activation-functions-with-derivative-and-python-code-sigmoid-vs-tanh-vs-relu-44d23915c1f4#:~:text=Like%20the%20sigmoid%20function%2C%20one%20of%20the%20interesting,of%20tanh%20%28z%29%3A%20a%3D%20%28e%5Ez-e%5E%20%28-z%29%29%2F%20%28e%5Ez%2Be%5E%20%28-z%29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Cost Functions used in Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  L1 and L2 loss \n",
    "\n",
    "> *L1* and *L2* are two common Loss functions which are mainly used to minimize the error.\n",
    "\n",
    "   *   **L1 loss function** is also known as **Least Absolute Deviations** in short **LAD**.\n",
    "\n",
    "   *   **L2 loss function** is also known as **Least Square Errors** in short **LSE**.\n",
    "\n",
    "### L1 Loss function\n",
    "\n",
    "> It is used to minimize the error which is the sum of all the absolute differences in between the Actual value and the Predicted value.\n",
    "<img src=\"images\\img13.png\" width='480px'>\n",
    "\n",
    "\n",
    "### L2 Loss Function\n",
    "\n",
    "> It is also used to minimize the error which is the sum of all the squared differences in between the Actual value and the Predicted value.\n",
    "<img src=\"images\\img15.png\" width='480px'>\n",
    "\n",
    "> **L1 Loss is more sensitive to outliers and controls them, so it is always advisable to use L1 Loss in case of outliers, otherwise go with L2 Loss.** \n",
    "\n",
    "###  Python Implementation for L1 and L2 losses\n",
    "\n",
    ">  **l1_loss = tf.abs((y_pred - y_actual))**\n",
    "  \n",
    ">  **l2_loss = tf.square((y_pred - y_actual))**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Huber Loss \n",
    "\n",
    "> Huber Loss is a combination of L1 & L2 losses and this uses a threshold value to detect the outliers.\n",
    "  In case of outliers, it uses L1 Loss with some regularizations otherwise it uses the L2 loss. \n",
    "\n",
    "<img src=\"images\\huber-loss.jpg\" width='480px'>  \n",
    "\n",
    "\n",
    "> Huber Loss is often used in Regression problems. Compared with L2 loss, Huber Loss is less sensitive to outliers (because if the residual is too large, it is a Piecewise function, loss is a linear function of the residual).\n",
    "Among them, $\\delta$ is a set parameter, $y$ represents the real value, and $f(x)$ represents the predicted value.\n",
    "\n",
    "> The advantage of this is that when the residual is small, the loss function is L2 norm, and when the residual is large, it is a linear function of L1 norm.\n",
    "\n",
    "> The Huber loss is quadratic when the error is smaller than a thres‐hold δ (typically 1), but linear when the error is larger than δ. Thismakes it less sensitive to outliers than the mean squared error, andit is often more precise and converges faster than the mean abso‐lute error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo-Huber Loss function \n",
    "\n",
    "> A smooth approximation of Huber loss to ensure that each order is differentiable.\n",
    "\n",
    "<img src=\"images\\img2.png\" width='780px'>\n",
    "\n",
    "> Where $\\delta$ is the set parameter, the larger the value, the steeper the linear part on both sides.\n",
    "\n",
    "<img src=\"images\\img3.png\" width='360px'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  Hinge Loss\n",
    " \n",
    "> Hinge loss is often used for Binary classification problems, such as ground true: t = 1 or -1, predicted value y = wx + b. \n",
    " \n",
    "> In other words, the closer the y is to t, the smaller the loss will be.\n",
    "\n",
    "<img src='images/hinge-loss.png'>\n",
    "\n",
    "> For getting more about [Hinge Loss ](https://en.wikipedia.org/wiki/Hinge_loss)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "###  Implementation of Hinge Loss in Python\n",
    "\n",
    "x_guess2 = tf.linspace(-3.,5.,500)\n",
    "x_actual2 = tf.convert_to_tensor([1.]*500)\n",
    "\n",
    "#Hinge loss\n",
    "#hinge_loss = tf.losses.hinge_loss(labels=x_actual2, logits=x_guess2)\n",
    "hinge_loss = tf.maximum(0.,1.-(x_guess2*x_actual2))\n",
    "0with tf.Session() as sess:\n",
    "    x_,hin_ = sess.run([x_guess2, hinge_loss])\n",
    "    plt.plot(x_,hin_,'--', label='hin_')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Cross Entropy Loss\n",
    "\n",
    "> Cross Entropy Loss is mainly applied to Binary classification problems. The predicted value is a probability value and the loss is defined according to the cross entropy. Note the value range of the above value: the predicted value of y should be a probability and the value range is [0,1] .\n",
    "\n",
    "<img src=\"images\\cross-entropy.jpg\" width='420px'>\n",
    "\n",
    "<img src=\"images\\img7.png\" width='420px'>\n",
    "<img src=\"images\\img8.png\" width='420px'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Sigmoid Cross-entropy Loss\n",
    "\n",
    "> The above cross-entropy loss requires that the predicted value is a probability. Generally, we calculate $scores = x*w + b$. Entering this value into the sigmoid function can compress the value range to (0,1).\n",
    "\n",
    "<img src=\"images\\img9.png\">\n",
    "\n",
    "> It can be seen that the sigmoid function smoothes the predicted value(such as directly inputting 0.1 and 0.01 and inputting 0.1, 0.01 sigmoid and then entering, the latter will obviously have a much smaller change value), which makes the predicted value of sigmoid-ce far from the label loss growth is not so steep.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Softmax Cross-entropy Loss\n",
    "\n",
    "> First, the softmax function can convert a set of fraction vectors into corresponding probability vectors. Here is the definition of softmax function\n",
    "\n",
    "<img src=\"images\\img10.png\">\n",
    "\n",
    "> As above, softmax also implements a vector of 'squashes' k-dimensional real value to the [0,1] range of k-dimensional, while ensuring that the cumulative sum is 1.\n",
    "\n",
    "> According to the definition of cross entropy, probability is required as input.Sigmoid-cross-entropy-loss uses sigmoid to convert the score vector into a probability vector, and softmax-cross-entropy-loss uses a softmax function to convert the score vector into a probability vector.\n",
    "\n",
    "> According to the definition of cross entropy loss.\n",
    "\n",
    "<img src=\"images\\img11.png\">\n",
    "\n",
    "> where $p(x)$ represents the probability that classification $x$ is a correct classification, and the value of $p$ can only be 0 or 1. This is the prior value $q(x)$ is the prediction probability that the $x$ category is a correct classification, and the value range is (0,1)\n",
    "\n",
    "> So specific to a classification problem with a total of C types, then $p(x_j)$, $(0 <_{=} j <_{=} C)$ must be only 1 and C-1 is 0(because there can be only one correct classification, correct the probability of classification as correct classification is 1, and the probability of the remaining classification as correct classification is 0)\n",
    "\n",
    "> Then the definition of softmax-cross-entropy-loss can be derived naturally.\n",
    "\n",
    "<img src=\"images\\img12.png\">\n",
    "\n",
    ">Where $f_j$ is the score of all possible categories, and $f_{y_i}$ is the score of ground true class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Role of Optimisers in Deep Learning\n",
    "\n",
    "> Optimisers played a vital role in Neural Networks training. When data enters inside the network with Input Nodes, Feed Forward Connection starts and it will end up with getting some losses.\n",
    "\n",
    "> Then Backward Propagation starts and it updates the randomly selected parameters layer-by-layer in the Network, by using some 'optimization' mechanism in backward direction.\n",
    "   \n",
    "<img src=\"images\\diag.jpg\" width='720px'>\n",
    "\n",
    ">  There are 3 basic optimisers based on **Gradient Descent** approach, that we uses in Deep learning.\n",
    " \n",
    "     *  Batch Gradient Descent or (BGD)\n",
    "     *  Stochastic Gradient Descent or (SGD)\n",
    "     *  Mini Batch Gradient Descent or (MBGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   Batch Gradient Descent\n",
    "\n",
    "> BGD uses the entire Training set to calculate the **Gradient** of the **Cost function** to the parameters.\n",
    "\n",
    "> In BGD, We try to send the whole dataset at a time and Records are selected randomly and enters to the network one-by-one. Then we calculate a Combined Loss or Cost by taking Summation of all losses, and then we send the Cost for optimisation for finding out the Gradients. \n",
    "\n",
    "> In BGD, it will be able to do **Optimisation**, if it will be able to pass entire dataset to the **Neural network**.\n",
    "\n",
    "> **Batch Gradient Descent can converge to a global minimum for convex functions and to a local minimum for non-convex functions.**\n",
    "\n",
    ">  **Disadvantages of BGD**\n",
    "\n",
    "* Memory Consumption is too high\n",
    "* Calculation of Gradients will be slow\n",
    "* Computationally very expensive.\n",
    "* Not good to update the model in real time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Mini-batch Gradient Descent\n",
    "\n",
    "> In MBGD, We make batches of 'n' (n ε (50 ～ 256 advisable) records and then these mini-batches are selected randomly and enters to the Neural network one-by-one. Then we calculates Costs for each mini-batches and send for **optimisation**.\n",
    "\n",
    "> In a single Iteration, one mini-batch passes thru the Network and we calculate the Cost and send it for **optimisation** and then update the parameters. Then in next Iteration, some other mini-batches will be passes and we calculates the cost and update the weights.\n",
    " \n",
    "> MBGD uses a small batch of samples, that is, n samples to calculate each time. In this way, it can reduce the variance when the parameters are updated, and the convergence is more stable.\n",
    " It can make full use of the highly optimized matrix operations in the **Deep Learning** library for more efficient gradient calculations.\n",
    "\n",
    "> **MBGD is modified version of BGD but with multiple mini-batches with 'n ε (50 ～ 256)' records.\n",
    "\n",
    "> **In MBGD, if batch size is equal to Total no. of records in datset, it becomes BGD.**\n",
    "\n",
    "> **Compare to BGD, MBGD is resource efficient, consumes low memory and calculation is faster. \n",
    " \n",
    "**Disadvantages with Mini-Batch Gradient Descent**\n",
    "\n",
    "> **Mini-batch Gradient Descent** does not gives a guarantee that we will be able to do a good convergence of Data or Error in a better way, causes due to randomly selection of batches, because Samples are extracted from dataset don't represents the properties of Entire dataset and so we don't get a good Convergence and so also not a **Absolute Global Minima or Local Minima** points.\n",
    "\n",
    "> If **Learning rate, η** is tto small, my convergence rate falls and Time to take for finding out the Absolute minima will increase and in case of a high **Learning rate, η**, it will not be able to achieve Absolute Mnima and it will keep oscillating between Maximum values and minimum values, causes due to different errors getting for different mini-batches.  \n",
    "\n",
    "> We should control the **Learning rate, η** in case of MBGD.\n",
    "> In addition, this method is to apply the **same learning rate** to all parameter updates. If our data is sparse, we would prefer to update the features with lower frequency.\n",
    "\n",
    "> In addition, for **non-convex functions**, it is also necessary to avoid trapping at the local minimum or saddle point, because the error around the saddle point is the same, the gradients of all dimensions are close to 0, and SGD is easily trapped here.\n",
    "\n",
    "> **Saddle points** are the curves, surfaces, or hypersurfaces of a saddle point neighborhood of a smooth function are located on different sides of a tangent to this point.\n",
    "For example, this two-dimensional figure looks like a saddle: it curves up in the x-axis direction and down in the y-axis direction, and the saddle point is (0,0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Stochastic Gradient Descent\n",
    "\n",
    "> In **Stochastic Gradient Descent**, a single record is selected and send to the **Neural networks**. We calculates the Loss for this single record and send it for **optimisation** and the update the weights.\n",
    " Then in next Iterations, some other records are sent to the network individually and we calculates the loss and the update the weights.\n",
    " \n",
    "> In SGD, **Loss function** and **optimisers** are not supposed to wait for the entire dataset to calculate themselves. \n",
    "\n",
    "> Comparing to BGD, SGD works just with a single iteration and so requires less computational resources but takes more time to train the network.\n",
    "\n",
    "> IN BGD, it will get converged to **Local Minima** point, but in SGD, it will always oscillates or vary between one point to other for each and every dataset, so it's very difficult to get **Absolute Minima** point.\n",
    "\n",
    "> IN SGD, gets multiple Minimum values for entire dataset and we get some **zig-zag** type curve and minima's will keep fluctuating.\n",
    "\n",
    "> For SGD, **Learning rate, η** should be lesser comparing to BGD and MBGd.\n",
    "\n",
    "> SGD is faster than BGD and MBGD.\n",
    "\n",
    "<img src='images/sgd.png' width='320px'>\n",
    " \n",
    " **<center>Fluctuations in SGD</center>** \n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "> However, because SGD is updated more frequently, the cost function will have severe oscillations.\n",
    "BGD can converge to a local minimum, of course, the oscillation of SGD may jump to a better local minimum.\n",
    "\n",
    "> When we decrease the learning rate slightly, the convergence of SGD and BGD is the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  BGD, MBGD and SGD are some ways to pass the data from Neural networks.\n",
    "\n",
    "<img src='images/gradient-update.jpg' width='480px'>\n",
    "\n",
    "###  Optimizers are ways to calculate the Gradients and update the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept of Momentum in Deep Learning\n",
    "\n",
    "> **Momentum is momentum**, which simulates the inertia of an object when it is moving, that is, the direction of the previous update is retained to a certain extent during the update, while the current update gradient is used to fine-tune the final update direction. In this way, you can increase the stability to a certain extent, so that you can learn faster, and also have the ability to get rid of local optimization.\n",
    "\n",
    "> Using **Momentum-based Optimizers**, We get smooth convergence of data and takes very less time to calculate the gradients.\n",
    "\n",
    "<img src='images/sgd1.png' width='480px'>\n",
    " \n",
    " **<center>Figure :- SGD without Momentum &&&  SGD with Momentum</center>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptive Momentum-based Gradient Descent optimiser or AdaGrad\n",
    "\n",
    "> **Adagrad** is an algorithm for gradient-based optimization, which adapts the **Learning rates** to the parameters, using low **Learning rate** for parameters associated with frequently occurring features, and using high **Learning rates** for parameters associated with infrequent features. So, it is well-suited for dealing with **sparse data**.\n",
    "\n",
    "> In very beginning, We starts from a high **Learning rate** and gradually as I will increase my Iterations, I will keep decreasing my **Learning rate** because in very beginning, our **Neural network** will not be aware about dataset and Every time we receives new variances or records of data, but in Later stage, once my **Neural network** is able to pass data for certain epochs (or Iterations) and so my model has alreday adjusted the weights and if their will be some changes in features are not frequent, we will slow down the **Learning rate**.\n",
    "In this case, I am providing my model a better possibility to converge for optimise the Losses.\n",
    "\n",
    "> I will be able to define the **Accuracy** of my model, if it will converge and this way, we uses to keep changing the **Learning rate** and will not use a constant **Learning rate** throughout the entire learning period.\n",
    "\n",
    "> A constant **Learning rate** may not be suitable for all parameters. For example, some parameters may have reached the stage where only fine-tuning is needed, but some parameters need to be adjusted a lot due to the small number of corresponding samples.\n",
    "\n",
    "> Adagrad proposed as an algorithm that adaptively assigns different **Learning rates** to various parameters among them. \n",
    "\n",
    ">**GloVe word embedding uses AdaFrad where infrequent words required a greater update and frequent words require smaller updates.**\n",
    "\n",
    ">**AdaGrad eliminates the need to manually tune the learning rate.**\n",
    "\n",
    "<img src='images/adagrad.jpg' width='480px'>\n",
    "\n",
    "##  Disadvantages in Adagrad optimiser\n",
    "\n",
    "* **The learning rate is monotonically decreasing.**\n",
    "* **The learning rate in the late training period is very small.**\n",
    "* **It requires manually setting a global initial learning rate.**\n",
    "* **Computationally very Expensive**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adadelta\n",
    "\n",
    "> **Adadelta** is an extension of **Adagrad** and it tries to reduce Adagrad’s property of aggressively and monotonically reducing the **Learning rate**.\n",
    "\n",
    "<img src='images/adadelta.jpg' width='480px'>\n",
    "\n",
    "> In **AdaGrad optimiser**, we uses summation of squares of gradients from all previous epochs, but In **Adadelta optimiser**, we uses **root-mean-square** of gradient of this current epoch and uses **Learning rate** from previous epoch. \n",
    "\n",
    ">In **Adadelta**, we take the ratio of the **Running average** of the previous epochs to the current gradient and we do not need to set the default **Learning rate**.\n",
    "\n",
    "> So,this way! I am able to eliminate the dependencies of selecting a Hard-coded value as **Learning rate** and this **Learning rate** is dynamic and will not be decreased to a small value because learning rate will be changed in each & every epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  RMSProp Optimiser\n",
    "\n",
    "> The full name of RMSProp algorithm is **Root Mean Square Propagation**, which is an adaptive learning rate optimization algorithm proposed by **Geoffrey Hinton**. \n",
    "\n",
    ">**Geoff Hinton** is known as **Father of Deep learning** and get **Alan Turing award** in 2018.\n",
    "\n",
    ">**RMSProp** tries to resolve Adagrad’s radically diminishing **Learning rates** by using a moving average of the squared gradient. It utilizes the magnitude of the recent gradient descents to normalize the gradient.\n",
    "\n",
    "<img src='images/rmsprop.jpg' width='480px'>\n",
    "\n",
    "> **Adagrad** will accumulate all previous gradient squares, but **RMSProp** just calculates the corresponding average value, so it can alleviate the problem that the learning rate of the **Adagrad algorithm** drops quickly.\n",
    "\n",
    "> The difference is that **RMSProp** calculates the **differential squared weighted average of the gradients** and this makes the network functions converge faster. \n",
    "\n",
    ">In **RMSProp**, **Learning rate** gets adjusted automatically and it chooses a different learning rate for each parameter.\n",
    "\n",
    ">**RMSProp** divides the learning rate by the average of the exponential decay of squared gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam\n",
    "\n",
    "**Adaptive Moment Estimation optimiser, Adam** is another method that computes adaptive learning rates for each parameter by storing an exponentially decaying average of past squared gradients like **Adadelta** and **RMSprop**.\n",
    "\n",
    "> **Adam** also keeps an exponentially decaying average of past gradients, similar to momentum.\n",
    "\n",
    "> **Adam** can be viewed as a combination of **Adagrad** and **RMSprop** so that **Adagrad** works well on sparse gradients and **RMSProp** works well in online and non-stationary settings repectively.\n",
    "\n",
    "> **Adam** implements the **Exponential moving average of the gradients** to scale the **Learning rate** instead of a simple average as in **Adagrad**. It keeps an exponentially decaying average of past gradients.\n",
    "\n",
    "> **Adam optimizer** is computationally efficient and has very less memory requirement and itis one of the most popular and famous **Gradient descent-based Optimization** algorithms.\n",
    "\n",
    "> **Adam optimizer** comes with **SGD**, by default but also can be used with other. \n",
    "\n",
    "<img src='images/adam.jpg' width='480 px'>\n",
    "<img src='images/adam1.jpg' width='480 px'>\n",
    "<img src='images/adam2.jpg' width=' 360px'>\n",
    "\n",
    "> **Uncentored Variances** are those variances which, are not able to be defined in both directions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **AdaMax Optimizer**, **N_Adam optimizer** & **MS Grad optimizer** are some more good optimizers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparisions\n",
    "\n",
    "<img src='images/comp.gif' width='480px'>\n",
    "\n",
    "**<center>Figure :- SGD optimization on loss surface contours</center>**\n",
    "\n",
    "<img src='images/sadd.gif' width='480px'>\n",
    "\n",
    "**<center>Figure :- SGD optimization on saddle point</center>**\n",
    "\n",
    "\n",
    "\n",
    "# How to choose optimizers?\n",
    "\n",
    "- If the data is sparse, use the self-applicable methods, namely Adagrad, Adadelta, RMSprop, Adam.\n",
    "\n",
    "- RMSprop, Adadelta, Adam have similar effects in many cases.\n",
    "\n",
    "- Adam just added bias-correction and momentum on the basis of RMSprop,\n",
    "\n",
    "- As the gradient becomes sparse, Adam will perform better than RMSprop.\n",
    "\n",
    "**Overall, Adam is the best choice.**\n",
    "\n",
    ">SGD is used in many papers, without momentum, etc. Although SGD can reach a minimum value, it takes longer than other algorithms and may be trapped in the saddle point.\n",
    "\n",
    "- If faster convergence is needed, or deeper and more complex neural networks are trained, an adaptive algorithm is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Multi-layered Perceptrons Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Architecture of a Neural network used for Regression\n",
    "\n",
    "> MLPs can be used for Regression tasks. If you want to predict a single value (e.g.,the price of a house), \n",
    "      then you just need a single output neuron, its output is the predicted value. \n",
    " \n",
    "> For Multivariate Regression or to predict multiple values at once, you need one output neuron per \n",
    "       output dimension.\n",
    "        \n",
    "> For example, to locate the center of an object on an image, you need to predict 2D coordi‐nates, \n",
    "        so\n",
    "        you need two output neurons. If you also want to place a bounding boxaround the object, then you need\n",
    "          two more neurons for the 'width' and 'height' of the object and you end up with 4 output neurons.\n",
    "            \n",
    "> In general, when building an MLP for Regression, you do not want to use any Activation function for \n",
    "      the output neurons, so they are free to output any range of values.\n",
    "     However, if you want to guarantee that the output will always be positive, then you can use the ReLU \n",
    "        activation function, or Softplus Activation function in the output layer.\n",
    "        \n",
    "> Finally, if you want to guarantee that the predictions will fall within a given range of values, then \n",
    "      you can use the Logistic function or the Hyperbolic Tangent, and scale the labels to the appropriate \n",
    "        range (0 to 1) for the logistic function, or (–1 to 1) for the Hyperbolic Tangent function.\n",
    "        \n",
    "> The Loss function to use during training is typically the Mean Squared Error, but if you have lot of outlier\n",
    "       in the training set, you may prefer to use the Mean Absolute Error instead. \n",
    "      Alternatively, you can use the Huber loss, which is a combination of both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/reg-mlp1.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Classification MLP Architecture\n",
    "\n",
    "> MLPs can also be used for Classification tasks. \n",
    "     \n",
    "> For a Binary classification problem, you just need a single output neuron using the Logistic Activation function and the output will be a number between (0 & 1), which you can interpret as the estimated probability of the Positive class. \n",
    " Obviously, the estimated probability of the Negative class is equal to one minus that number. \n",
    "\n",
    "> MLPs can also easily handle multi-labeled Binary classification tasks. \n",
    "     For example, you could have an email classification system that predicts whether each incoming email is \n",
    "     ham or spam, and simultaneously predicts whether it is an urgent or non-urgent email. \n",
    "\n",
    "> In this case, you would need two Output neurons, both using the Logistic Activation function. \n",
    "    The first would output the probability that the email is spam and the second would output the probability that\n",
    "      it is urgent. More generally, you would dedicate one output neuron for each Positive class. \n",
    "    Note that the output probabilities do not necessarily add up to one. This lets the model output any combination\n",
    "      of labels. You can have non-urgent ham, urgent ham, non-urgent spam, and perhaps even urgent spam \n",
    "        (although that would probably be an error).\n",
    " \n",
    "> If each instance can belong only to a single class, out of 3 or more possible classes\n",
    "      (e.g., classes 0 through 9 for digit image classification), then you need to have one output neuron per class,\n",
    "        and you should use the Softmax Activation function for the whole output layer. \n",
    " \n",
    "> The Softmax function will ensure that all the estimated probabilities are between 0 and 1 and that they add up\n",
    "      to one (which is required if the classes are exclusive). This is called Multiclass classification.\n",
    "\n",
    ">  As Loss function, Cross - entropy (or Log loss) function is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/clf-mlp1.png'>\n",
    "<img src='images/clf1.png'>\n",
    "<img src='images/clf2.png'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
